{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffeaafc8-504b-48c2-b791-0f4dac03da9f",
   "metadata": {},
   "source": [
    "# REALEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a473cec-727a-402f-aa5d-c5c71484f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elisabetta/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "/home/elisabetta/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import thesis_utils\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953a4bf7-97c3-4a14-b188-d262de5b1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_ids_tokens_gold (list_sentences):\n",
    "    all_sentences_info = []\n",
    "    \n",
    "    for idx, sentence in enumerate(list_sentences):\n",
    "        sent_info = []\n",
    "        for idx_tok, line in enumerate(sentence):\n",
    "            tok_tuple = (f'T{idx_tok}',line[0],line[-1])\n",
    "            sent_info.append(tok_tuple)\n",
    "            \n",
    "        all_sentences_info.append(sent_info)\n",
    "    \n",
    "    return all_sentences_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35e973a-e548-4d24-8f65-7c7c055d3e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067\n"
     ]
    }
   ],
   "source": [
    "multiged_realec_dev =  thesis_utils.read_tsv_file_and_find_sentences_without_headers('./MULTI-GED2023 DATA/en_realec_dev.tsv')\n",
    "multiged_dev_info= get_list_ids_tokens_gold(multiged_realec_dev)\n",
    "print(len(multiged_dev_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7081211c-590f-4468-97ea-affb8b23a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiged_realec_test =  thesis_utils.read_tsv_file_and_find_sentences('./MULTI-GED2023 DATA/en_realec_test_unlabelled.tsv')\n",
    "# multiged_test_tokens = [[token[0] for token in sentence] for sentence in multiged_realec_test]\n",
    "# print(len(multiged_realec_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa0cc24-6c64-4464-9ff4-dceaeb53b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_multiged(sentences_tokens_lists):\n",
    "    for sentence in sentences_tokens_lists:\n",
    "        for i, (idx, token, label) in enumerate(sentence):\n",
    "            if token.startswith('\\\\'):\n",
    "                cleaned_token = token.lstrip('\\\\')\n",
    "                sentence[i] = (idx, cleaned_token, label)\n",
    "    return sentences_tokens_lists\n",
    "\n",
    "\n",
    "multiged_dev_info = clean_multiged(multiged_dev_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa2168e-77cb-418d-8399-6bcb5ea39bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "realec_dataset = './exam/Exam_to_examine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c65c51f-943e-4b65-869a-55acd6925db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a5634e6-6bda-40bd-8047-03fe7025a952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing .txt for: ./exam/Exam_to_examine/EGe_100-199/2017_EGe_13_2.ann, skipping.\n",
      "113584\n"
     ]
    }
   ],
   "source": [
    "def find_overlapping_spans(token_start, token_end, annotations):\n",
    "    overlapping = []\n",
    "    for ann in annotations:\n",
    "        if not (token_end <= ann['start'] or token_start >= ann['end']):\n",
    "            overlapping.append(ann)\n",
    "    return overlapping\n",
    "\n",
    "wrong_re = re.compile(r'^T(\\d+)\\s+([a-zA-Z_]+)\\s+(\\d+)\\s+(\\d+)\\s+(.+)$', re.IGNORECASE)\n",
    "correct_re = re.compile(r'^#(\\d+)\\s+AnnotatorNotes\\s+T(\\d+)\\s+(.+)$', re.IGNORECASE)\n",
    "\n",
    "all_sentences = []\n",
    "max_error_pairs = 0\n",
    "\n",
    "for root_dir, _, files in os.walk(realec_dataset):\n",
    "    for file in files:\n",
    "        if '.ipynb_checkpoints' in root_dir or '-checkpoint' in file or not file.endswith('.ann'):\n",
    "            continue\n",
    "\n",
    "        ann_path = os.path.join(root_dir, file)\n",
    "        txt_path = ann_path.replace('.ann', '.txt')\n",
    "\n",
    "        if not os.path.exists(txt_path):\n",
    "            print(f\"Missing .txt for: {ann_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f_txt, \\\n",
    "             open(ann_path, 'r', encoding='utf-8') as f_ann:\n",
    "\n",
    "            txt_content = f_txt.read().replace('\\n', ' ')\n",
    "            txt_content = txt_content.replace('\\t', ' ')\n",
    "            doc = nlp(txt_content)\n",
    "\n",
    "            error_annotations = []\n",
    "            lines = f_ann.read().splitlines()\n",
    "\n",
    "            i = 0\n",
    "            while i < len(lines):\n",
    "                line = lines[i].strip()\n",
    "                if not line or (line.startswith('#') and 'lemma =' in line.lower()):\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                wrong_match = wrong_re.match(line)\n",
    "                if wrong_match:\n",
    "                    error_id = wrong_match.group(1)\n",
    "                    error_type = wrong_match.group(2)\n",
    "                    if error_type.isupper():\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                    wrong_beg = int(wrong_match.group(3))\n",
    "                    wrong_end = int(wrong_match.group(4))\n",
    "                    wrong_text = wrong_match.group(5)\n",
    "                    correction = \"\"\n",
    "\n",
    "                    if i + 1 < len(lines):\n",
    "                        next_line = lines[i + 1].strip()\n",
    "                        if not (next_line.startswith('#') and 'lemma =' in next_line.lower()):\n",
    "                            correct_match = correct_re.match(next_line)\n",
    "                            if correct_match and correct_match.group(2) == error_id:\n",
    "                                correction = correct_match.group(3)\n",
    "                                i += 1\n",
    "\n",
    "                    error_annotations.append({\n",
    "                        'start': wrong_beg,\n",
    "                        'end': wrong_end,\n",
    "                        'type': error_type,\n",
    "                        'correction': correction\n",
    "                    })\n",
    "                i += 1\n",
    "\n",
    "            token_id = 0\n",
    "            for sent in doc.sents:\n",
    "                sent_tokens = []\n",
    "                for token in sent:\n",
    "                    overlapping_errors = find_overlapping_spans(token.idx, token.idx + len(token.text), error_annotations)\n",
    "                    max_error_pairs = max(max_error_pairs, len(overlapping_errors))\n",
    "\n",
    "                    token_data = [file, f\"T{token_id}\", token.text]\n",
    "                    token_id += 1\n",
    "                    \n",
    "                    for error in overlapping_errors:\n",
    "                        token_data.extend([error['type'], error['correction']])\n",
    "\n",
    "                    sent_tokens.append(token_data)\n",
    "\n",
    "                all_sentences.append(sent_tokens)\n",
    "\n",
    "for sent in all_sentences:\n",
    "    for token_data in sent:\n",
    "        while len(token_data) < 3 + max_error_pairs * 2:\n",
    "            token_data.extend(['', ''])\n",
    "\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7482d547-f59f-4c73-8354-4764dcb1c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2019_ABu_241_1.ann', 'T0', 'Three', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T1', 'chats', 'Spelling', 'charts', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T2', 'indicate', 'lex_item_choice', 'show', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T3', 'the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T4', 'proportion', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T5', 'of', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T6', 'people', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T7', 'of', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T8', 'different', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T9', 'ages', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T10', 'having', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T11', 'science', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T12', ',', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T13', 'art', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T14', 'or', 'Punctuation', ', or', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T15', 'sports', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T16', 'and', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T17', 'health', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T18', 'courses', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T19', 'in', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T20', '2012', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['2019_ABu_241_1.ann', 'T21', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]\n"
     ]
    }
   ],
   "source": [
    "print(all_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f1ccc7c-c122-4bbd-a357-708555754580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_and_renumber(sentences):\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        new_sentence = []\n",
    "        token_counter = 0  # Reset for each sentence\n",
    "\n",
    "        for row in sentence:\n",
    "            file_name, _, token_text = row[:3]\n",
    "            other_fields = row[3:]\n",
    "\n",
    "            if token_text.startswith('-') and len(token_text) > 1:\n",
    "                # Split into '-' and the rest\n",
    "                minus_token = [file_name, f'T{token_counter}', '-', *other_fields]\n",
    "                token_counter += 1\n",
    "                rest_token = [file_name, f'T{token_counter}', token_text[1:], *other_fields]\n",
    "                token_counter += 1\n",
    "                new_sentence.extend([minus_token, rest_token])\n",
    "            else:\n",
    "                new_token = [file_name, f'T{token_counter}', token_text, *other_fields]\n",
    "                token_counter += 1\n",
    "                new_sentence.append(new_token)\n",
    "\n",
    "        processed_sentences.append(new_sentence)\n",
    "\n",
    "    return processed_sentences\n",
    "\n",
    "\n",
    "idk_if_it_works=split_tokens_and_renumber(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc52eaaf-b201-434e-84a7-e206a58cd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = []\n",
    "merged_sentences = []\n",
    "\n",
    "for sent in idk_if_it_works:\n",
    "    if sent and sent[-1][2].strip() == '':\n",
    "        sent = sent[:-1]\n",
    "    cleaned_sentences.append(sent)\n",
    "\n",
    "n = len(cleaned_sentences)\n",
    "i = 0\n",
    "\n",
    "while i < n:\n",
    "    sentence = cleaned_sentences[i]\n",
    "    merged_sentences.append(sentence)\n",
    "\n",
    "    if i + 1 < n:\n",
    "        merged_2 = sentence + cleaned_sentences[i + 1]\n",
    "        merged_sentences.append(merged_2)\n",
    "\n",
    "    if i + 2 < n:\n",
    "        merged_3 = sentence + cleaned_sentences[i + 1] + cleaned_sentences[i + 2]\n",
    "        merged_sentences.append(merged_3)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7f1536-2cc0-46c2-a9df-b1753ec08414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_realec_to_multiged(sentences_realec, multiged_tokens):\n",
    "    token_sequence_to_realec_occurrences = defaultdict(list)\n",
    "    \n",
    "    for realec_sentence in sentences_realec:\n",
    "        token_sequence = tuple(token_info[2] for token_info in realec_sentence)\n",
    "        token_sequence_to_realec_occurrences[token_sequence].append(realec_sentence)\n",
    "\n",
    "    multiged_sequence_counts = Counter(tuple(token[1] for token in sent) for sent in multiged_tokens)\n",
    "\n",
    "    matched_sentences = []\n",
    "    matched_sequences = set()\n",
    "\n",
    "    for token_sequence, realec_occurrences in token_sequence_to_realec_occurrences.items():\n",
    "        multiged_count = multiged_sequence_counts.get(token_sequence, 0)\n",
    "\n",
    "        if multiged_count > 0:\n",
    "            used = 0\n",
    "            for multiged_sent in multiged_tokens:\n",
    "                if tuple(token[1] for token in multiged_sent) == token_sequence:\n",
    "                    if used >= len(realec_occurrences):\n",
    "                        break\n",
    "                    realec_sentence = realec_occurrences[used]\n",
    "\n",
    "                    realec_with_labels = [\n",
    "                        tuple(token_info) + (multiged_label,)\n",
    "                        for token_info, (_, _, multiged_label) in zip(realec_sentence, multiged_sent)\n",
    "                    ]\n",
    "\n",
    "                    matched_sentences.append(realec_with_labels)\n",
    "                    used += 1\n",
    "                    multiged_count -= 1\n",
    "                    matched_sequences.add(token_sequence)\n",
    "\n",
    "    return matched_sentences, matched_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "383739a1-59d6-4259-bdbd-74f419dbdf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matched dev sentences: 4015\n",
      "Dev sentences still to match: 52\n",
      "Total unmatched REALEC sentences: 335354\n"
     ]
    }
   ],
   "source": [
    "dev_matches_spacy, dev_sequences_spacy = match_realec_to_multiged(merged_sentences, multiged_dev_info)\n",
    "\n",
    "print(f'Number of matched dev sentences: {len(dev_matches_spacy)}')       \n",
    "\n",
    "all_matched_sequences_spacy = dev_sequences_spacy #| test_sequences_spacy\n",
    "\n",
    "unmatched_multiged_dev_spacy = []\n",
    "for sentence in multiged_dev_info:\n",
    "    token_sequence = tuple(token[1] for token in sentence)\n",
    "    if token_sequence not in all_matched_sequences_spacy:\n",
    "        unmatched_multiged_dev_spacy.append(sentence)\n",
    "\n",
    "print(f'Dev sentences still to match: {len(unmatched_multiged_dev_spacy)}')\n",
    "\n",
    "\n",
    "unmatched_realec_sentences = []\n",
    "for realec_sentence in merged_sentences:\n",
    "    token_sequence = tuple(token_info[2] for token_info in realec_sentence)\n",
    "    if token_sequence not in all_matched_sequences_spacy:\n",
    "        unmatched_realec_sentences.append(realec_sentence)\n",
    "\n",
    "print(f\"Total unmatched REALEC sentences: {len(unmatched_realec_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a9a12e-4a72-49f4-90b9-f1b62bf2c4d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('2019_ABu_42_2.ann', 'T0', 'To', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T1', 'sum', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T2', 'up', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T3', ',', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T4', 'I', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T5', 'would', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T6', 'like', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T7', 'to', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T8', 'say', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T9', 'that', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T10', ',', 'Punctuation', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T11', 'in', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T12', 'general', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T13', ',', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T14', 'governments', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T15', 'should', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T16', 'focus', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T17', 'on', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T18', 'their', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T19', 'countries', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T20', ',', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T21', 'but', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T22', 'there', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T23', 'are', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T24', 'such', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T25', 'cases', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T26', 'when', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T27', 'it', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T28', 'very', 'Absence_comp_sent', 'is very necessary', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2019_ABu_42_2.ann', 'T29', 'necessary', 'Absence_comp_sent', 'is very necessary', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T30', 'to', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T31', 'pay', 'lex_part_choice', 'pay attention to', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T32', 'attention', 'lex_part_choice', 'pay attention to', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2019_ABu_42_2.ann', 'T33', 'on', 'lex_part_choice', 'pay attention to', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2019_ABu_42_2.ann', 'T34', 'other', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T35', 'nations', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T36', 'and', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T37', 'help', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T38', 'them', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_42_2.ann', 'T39', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c')], [('2019_ABu_193_1.ann', 'T0', 'As', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T1', 'for', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T2', 'Australia', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T3', ',', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T4', 'the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T5', 'greatest', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T6', 'number', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T7', 'of', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T8', 'employees', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T9', 'of', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T10', 'both', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T11', 'genders', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T12', 'work', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T13', 'in', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T14', 'the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T15', 'Services', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T16', 'sector', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T17', 'as', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T18', 'well', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2019_ABu_193_1.ann', 'T19', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c')]]\n"
     ]
    }
   ],
   "source": [
    "print(dev_matches_spacy[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21167bab-5f2f-44b7-b168-fc2b1a6fe344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_idx_and_filenames(list_of_lines_for_sentence):\n",
    "    all_sentences_tok = []\n",
    "    all_indexes= []\n",
    "    all_filenames = []\n",
    "    \n",
    "    for sent in list_of_lines_for_sentence:\n",
    "        sent_tok = []\n",
    "        sent_idx = []\n",
    "        sent_nam = []\n",
    "        for line in sent:\n",
    "            file_name = line[0]\n",
    "            tok_idx=line[1]\n",
    "            token = line[2]\n",
    "            sent_tok.append(token)\n",
    "            sent_idx.append(tok_idx)\n",
    "            sent_nam.append(file_name)\n",
    "        all_sentences_tok.append(sent_tok)\n",
    "        all_indexes.append(sent_idx)\n",
    "        all_filenames.append(sent_nam)\n",
    "    return all_sentences_tok,all_indexes,all_filenames\n",
    "    \n",
    "all_unmatched_dev_tok_sent,all_unmatched_realec_indexes, all_unmatched_realec_filenames = extract_tokens_idx_and_filenames(unmatched_realec_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29b50ca3-2295-4e6a-a503-54d761166ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unmatched_dev_tok_sent = []\n",
    "filtered_unmatched_realec_indexes = []\n",
    "filtered_unmatched_realec_filenames = []\n",
    "\n",
    "for sentence, idx, name in zip(all_unmatched_dev_tok_sent, all_unmatched_realec_indexes, all_unmatched_realec_filenames):\n",
    "    if len(sentence) > 0 and len(idx)>0 and len(name) > 0:\n",
    "        filtered_unmatched_dev_tok_sent.append(sentence)\n",
    "        filtered_unmatched_realec_indexes.append(idx)\n",
    "        filtered_unmatched_realec_filenames.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17338247-e870-47a7-a4c5-db730be52721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_as_single_strings_multiged_dev = []\n",
    "\n",
    "for sentence in unmatched_multiged_dev_spacy:\n",
    "    cleaned_sentence = ''\n",
    "    sentence_indices = ''\n",
    "    cleaned_labels = ''\n",
    "\n",
    "    for token in sentence:\n",
    "        token_text = token[1]\n",
    "        token_idx = token[0]\n",
    "        token_label = token[-1]\n",
    "\n",
    "        if token_text == '\\\\\"':\n",
    "            cleaned_sentence += '\"'\n",
    "        else:\n",
    "            cleaned_sentence += token_text\n",
    "\n",
    "        sentence_indices += token_idx * len(token_text)\n",
    "\n",
    "        cleaned_labels = ''.join(token[-1].replace('\\\\\\\\\\\\', '') for token in sentence)\n",
    "\n",
    "\n",
    "    sentences_as_single_strings_multiged_dev.append((sentence_indices,cleaned_sentence, cleaned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14340e52-bc2c-4594-ba1a-9e68fb324886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T0T0T0T0T0T0T1T1T2T2T2T2T2T2T2T3T3T4T4T4T4T4T4T4T5T5T5T6T6T6T6T6T6T7T7T7T8T9T9T9T9T9T9T9T9T9T9T10T10T10T11T11T12T12T12T12T12T12T12T12T13T13T13T14T14T14T14T14T14T15T15T15T15T16T16T16T17', \"Spring'smonthesinYakutskarefreezytoo,tempreturecanbepossiblenothigherthen0'C.\", 'ccicccicciccicccic')\n"
     ]
    }
   ],
   "source": [
    "print(sentences_as_single_strings_multiged_dev[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27f5f2a2-f5f3-4902-a135-0cbf204ae3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_as_single_strings_realec = []\n",
    "prev_cleaned = None\n",
    "prev_clean_indices = None\n",
    "prev_filename = None\n",
    "\n",
    "for sentence, idx_list, filename in zip(filtered_unmatched_dev_tok_sent, filtered_unmatched_realec_indexes, filtered_unmatched_realec_filenames):\n",
    "    cleaned = ''\n",
    "    clean_indices = ''\n",
    "    \n",
    "    for idx, token in zip(idx_list, sentence):\n",
    "        clean_token = token.replace('\\\\\\\\\\\\', '')\n",
    "        cleaned += clean_token\n",
    "        clean_indices += idx * len(clean_token)\n",
    "\n",
    "    sentences_as_single_strings_realec.append((filename[0], cleaned, clean_indices))\n",
    "    \n",
    "    if prev_cleaned is not None and prev_clean_indices is not None:\n",
    "        combined_cleaned = prev_cleaned + cleaned\n",
    "        combined_indices = prev_clean_indices + clean_indices\n",
    "        sentences_as_single_strings_realec.append((prev_filename[0], combined_cleaned, combined_indices))\n",
    "    \n",
    "    prev_cleaned = cleaned\n",
    "    prev_clean_indices = clean_indices\n",
    "    prev_filename = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5172b61b-0da8-4d40-bd3f-769005d6d7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_most_possible_multi_idx = list()\n",
    "possible = []\n",
    "matched_sentences = set()\n",
    "for name, sentence1, idx in sentences_as_single_strings_realec:\n",
    "    exact_match_found = False\n",
    "    \n",
    "    for multi_idx, sentence2, label in sentences_as_single_strings_multiged_dev:\n",
    "        if sentence1 == sentence2:\n",
    "            set_most_possible_multi_idx.append((name, idx, sentence1, label))\n",
    "            matched_sentences.add((sentence2, label))\n",
    "            exact_match_found = True\n",
    "            break\n",
    "    \n",
    "    if not exact_match_found:\n",
    "        for sentence2 in sentences_as_single_strings_multiged_dev:\n",
    "            if sentence2 in matched_sentences:\n",
    "                continue\n",
    "            if sentence1 in sentence2:\n",
    "                if abs(len(sentence2) - len(sentence1)) > 4:\n",
    "                    continue\n",
    "                possible.append((name, sentence1, sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd57a41-4fb2-4daa-a769-f054771619bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(set_most_possible_multi_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d90298f-1e66-4073-8469-0b48f075f032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2017_NMya_21_1.ann', 'T0T0T0T0T0T0T0T0T0T1T1T2T2T2T3T3T3T3T3T3T3T3T3T4T5T5T5T5T5T5T5T5T6T6T7T7T7T8T8T8T8T9T9T9T9T9T9T9T10T10T10T10T10T10T11T11T11T11T11T11T11T12T12T13T13T13T14T14T15T15', 'Accordingtothestatistic,FacebookisthemostpopularsocialnetworkintheU.S.', 'ccciccccccccccccc')\n",
      "\n",
      "('2017_NMya_8_1.ann', 'T0T0T0T1T1T1T1T1T1T1T1T1T2T2T2T3T3T3T3T3T4T4T4T4T4T4T4T4T4T4T4T5T5T5T6T6T6T7T7T8T8T8T8T8T9T9T9T9T9T9T10T10T10T10T10T10T10T10T11T12T12T12T12T12T12T12T12T13T14T14T14T14T14T14T14T14T14T15T15T15T16T16T16T16T16T16T16T16T17T17T17T17T17T18T18T18T18T19T19T19T19T19T19T20T20T20T20T20T20T20T21T21T21T21T22T23T23T23T24T24T24T24T24T24T25', 'Thefollowingbarchartillustratestheuseofmajorsocialnetworks:Facebook,InstagramandLinkedInamongU.S.adultsdividedinto4agegroups.', 'cccccccccccccccccciiccccccc')\n",
      "\n",
      "('2017_EGe_247_1.ann', 'T0T0T0T0T0T0T1T1T2T2T2T2T2T2T2T3T3T4T4T4T4T4T4T4T5T5T5T6T6T6T6T6T6T7T7T7T8T9T9T9T9T9T9T9T9T9T9T10T10T10T11T11T12T12T12T12T12T12T12T12T13T13T13T14T14T14T14T14T14T15T15T15T15T16T16T16T16', \"Spring'smonthesinYakutskarefreezytoo,tempreturecanbepossiblenothigherthen0'C.\", 'ccicccicciccicccic')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in set_most_possible_multi_idx:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f562808f-1590-4e94-b337-6b88d2e2721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_list):\n",
    "    output = []\n",
    "    \n",
    "    for data in data_list:\n",
    "        filename, indices, sentence, gold_labels = data\n",
    "        \n",
    "        numeric_indices = []\n",
    "        i = 0\n",
    "        while i < len(indices):\n",
    "            if indices[i] == 'T':\n",
    "                i += 1\n",
    "                num_str = ''\n",
    "                while i < len(indices) and indices[i].isdigit():\n",
    "                    num_str += indices[i]\n",
    "                    i += 1\n",
    "                if num_str:\n",
    "                    numeric_indices.append(num_str)\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        grouped_indices = []\n",
    "        if not numeric_indices:\n",
    "            continue\n",
    "        \n",
    "        current_group = [numeric_indices[0]]\n",
    "        for num in numeric_indices[1:]:\n",
    "            if num == current_group[-1]:\n",
    "                current_group.append(num)\n",
    "            else:\n",
    "                grouped_indices.append(current_group)\n",
    "                current_group = [num]\n",
    "        grouped_indices.append(current_group)\n",
    "        \n",
    "        start = 0\n",
    "        word_index_pairs = []\n",
    "        \n",
    "        for idx, group in enumerate(grouped_indices):\n",
    "            word_length = len(group)\n",
    "            if start + word_length > len(sentence):\n",
    "                break \n",
    "            word = sentence[start:start + word_length]\n",
    "            unique_index = f\"T{group[0]}\"\n",
    "            label = gold_labels[idx]\n",
    "            word_index_pairs.append([unique_index, word, label])\n",
    "            start += word_length\n",
    "        \n",
    "        output.append({filename: word_index_pairs})\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f018414a-c32b-4453-b5f1-e0e4341a043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_most_possible_matches_realec = process_data(set_most_possible_realec_idx)\n",
    "processed_most_possible_matches= process_data(set_most_possible_multi_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae09d7ac-b37d-495b-ba58-4db3bffc82e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'2017_NMya_21_1.ann': [['T0', 'According', 'c'], ['T1', 'to', 'c'], ['T2', 'the', 'c'], ['T3', 'statistic', 'i'], ['T4', ',', 'c'], ['T5', 'Facebook', 'c'], ['T6', 'is', 'c'], ['T7', 'the', 'c'], ['T8', 'most', 'c'], ['T9', 'popular', 'c'], ['T10', 'social', 'c'], ['T11', 'network', 'c'], ['T12', 'in', 'c'], ['T13', 'the', 'c'], ['T14', 'U.', 'c'], ['T15', 'S.', 'c']]}, {'2017_NMya_8_1.ann': [['T0', 'The', 'c'], ['T1', 'following', 'c'], ['T2', 'bar', 'c'], ['T3', 'chart', 'c'], ['T4', 'illustrates', 'c'], ['T5', 'the', 'c'], ['T6', 'use', 'c'], ['T7', 'of', 'c'], ['T8', 'major', 'c'], ['T9', 'social', 'c'], ['T10', 'networks', 'c'], ['T11', ':', 'c'], ['T12', 'Facebook', 'c'], ['T13', ',', 'c'], ['T14', 'Instagram', 'c'], ['T15', 'and', 'c'], ['T16', 'LinkedIn', 'c'], ['T17', 'among', 'c'], ['T18', 'U.S.', 'i'], ['T19', 'adults', 'i'], ['T20', 'divided', 'c'], ['T21', 'into', 'c'], ['T22', '4', 'c'], ['T23', 'age', 'c'], ['T24', 'groups', 'c'], ['T25', '.', 'c']]}, {'2017_EGe_247_1.ann': [['T0', 'Spring', 'c'], ['T1', \"'s\", 'c'], ['T2', 'monthes', 'i'], ['T3', 'in', 'c'], ['T4', 'Yakutsk', 'c'], ['T5', 'are', 'c'], ['T6', 'freezy', 'i'], ['T7', 'too', 'c'], ['T8', ',', 'c'], ['T9', 'tempreture', 'i'], ['T10', 'can', 'c'], ['T11', 'be', 'c'], ['T12', 'possible', 'i'], ['T13', 'not', 'c'], ['T14', 'higher', 'c'], ['T15', 'then', 'c'], ['T16', \"0'C.\", 'i']]}]\n"
     ]
    }
   ],
   "source": [
    "print(processed_most_possible_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4073d8e8-f895-4293-92d4-9757666d3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = []\n",
    "\n",
    "for pr_sentence in processed_most_possible_matches:\n",
    "    for pr_filename, pr_lines in pr_sentence.items():  # Unpack filename and actual lines\n",
    "        for dev_sentence in unmatched_realec_sentences:\n",
    "            matched_sentence = []\n",
    "            for dev_line in dev_sentence:\n",
    "                for pr_line in pr_lines:\n",
    "                    if (\n",
    "                        pr_filename == dev_line[0] and   # filename\n",
    "                        pr_line[0] == dev_line[1] and    # token id\n",
    "                        pr_line[1] == dev_line[2]        # word\n",
    "                    ):\n",
    "                        extended_line = dev_line + [pr_line[-1]]\n",
    "                        matched_sentence.append(tuple(extended_line))\n",
    "                        break  # Optional: avoid duplicate matches per dev_line\n",
    "            if matched_sentence:\n",
    "                all_.append(matched_sentence)\n",
    "                break  # Stop comparing this pr_sentence after first match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b05ea59-7ec0-4e5e-bbd4-51665d72f4c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('2017_NMya_21_1.ann', 'T0', 'According', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T1', 'to', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T2', 'the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T3', 'statistic', 'Spelling', 'statistics', 'Category_confusion', 'statistics', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2017_NMya_21_1.ann', 'T4', ',', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T5', 'Facebook', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T6', 'is', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T7', 'the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T8', 'most', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T9', 'popular', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T10', 'social', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T11', 'network', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T12', 'in', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T13', 'the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T14', 'U.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_21_1.ann', 'T15', 'S.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c')], [('2017_NMya_8_1.ann', 'T0', 'The', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T1', 'following', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T2', 'bar', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T3', 'chart', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T4', 'illustrates', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T5', 'the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T6', 'use', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T7', 'of', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T8', 'major', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T9', 'social', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T10', 'networks', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T11', ':', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T12', 'Facebook', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T13', ',', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T14', 'Instagram', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T15', 'and', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T16', 'LinkedIn', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T17', 'among', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T18', 'U.S.', 'Category_confusion', 'American', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2017_NMya_8_1.ann', 'T19', 'adults', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2017_NMya_8_1.ann', 'T20', 'divided', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T21', 'into', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T22', '4', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T23', 'age', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T24', 'groups', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_NMya_8_1.ann', 'T25', '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c')], [('2017_EGe_247_1.ann', 'T0', 'Spring', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T1', \"'s\", '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T2', 'monthes', 'Spelling', 'months', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2017_EGe_247_1.ann', 'T3', 'in', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T4', 'Yakutsk', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T5', 'are', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T6', 'freezy', 'Spelling', 'freezing', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2017_EGe_247_1.ann', 'T7', 'too', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T8', ',', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T9', 'tempreture', 'Spelling', 'temperature', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2017_EGe_247_1.ann', 'T10', 'can', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T11', 'be', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T12', 'possible', 'Category_confusion', 'possibly', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i'), ('2017_EGe_247_1.ann', 'T13', 'not', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T14', 'higher', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T15', 'then', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'c'), ('2017_EGe_247_1.ann', 'T16', \"0'C.\", 'Spelling', '0°C', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'i')]]\n"
     ]
    }
   ],
   "source": [
    "print(all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db7ddee4-d957-414e-82aa-d5a2b385dbea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_matches_spacy.extend(all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba7ff56f-92c5-4de2-a241-b77fef1769cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4015\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fdcbd2c1-fd8a-4f1d-af74-fa2490356b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[list(token) for token in sentence] for sentence in dev_matches_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f88ae209-fc80-45e7-b189-fc3fac6c246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_columns = len(dev_matches_spacy[0][0])\n",
    "\n",
    "# headers = ['filename', 'token'] + [f'col{i}' for i in range(1, num_columns - 11)] + ['gold_label']\n",
    "\n",
    "# with open('processed_realec.tsv', 'w', newline='', encoding='utf-8') as f_out:\n",
    "#     writer = csv.writer(f_out, delimiter='\\t')\n",
    "#     writer.writerow(headers)\n",
    "\n",
    "#     for sentence in dev_matches_spacy:\n",
    "#         for row in sentence:\n",
    "#             row_without_id = [row[0], row[2]] + list(row[3:])\n",
    "#             writer.writerow(row_without_id)\n",
    "#         writer.writerow([])\n",
    "\n",
    "with open(\"processed_realec.tsv\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "    headers = [\"filename\", \"token\"] + [f\"col{i+1}\" for i in range(8)] + [\"gold_label\"]\n",
    "    f_out.write(\"\\t\".join(headers) + \"\\n\")\n",
    "\n",
    "    for sentence in dev_matches_spacy:\n",
    "        for token in sentence:\n",
    "            filename = token[0]\n",
    "            word = token[2]\n",
    "            col_values = list(token[3:11])  # converte la slice in lista\n",
    "            gold_label = token[-1]\n",
    "\n",
    "            row = [filename, word] + col_values + [gold_label]\n",
    "            f_out.write(\"\\t\".join(row) + \"\\n\")\n",
    "        \n",
    "        f_out.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16de381-f850-427c-81a4-c3eb86f694ea",
   "metadata": {},
   "source": [
    "### from collections import Counter\n",
    "\n",
    "column_counts = Counter()\n",
    "\n",
    "with open(\"processed_realec.tsv\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            num_cols = len(line.strip().split('\\t'))\n",
    "            column_counts[num_cols] += 1\n",
    "\n",
    "print(\"Column count frequencies:\")\n",
    "for num_cols, count in sorted(column_counts.items()):\n",
    "    print(f\"{num_cols} columns: {count} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2bbf894-ad17-459f-a8f8-aa872a6fa7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#there are 49 senteces not aligned. the others are fixed\n",
    "sentences=[]\n",
    "for sentence in unmatched_multiged_dev_spacy:\n",
    "    sent_token = [tuple[1] for tuple in sentence]\n",
    "    sentences.append(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0c34855c-d8dc-4a4c-8a16-d59bc10fc562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "128bb071-c0ca-466f-bb93-eced747799a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'sum', 'up', ',', 'all', 'three', 'trends', 'went', 'up', 'and', 'the', 'number', 'of', 'people', 'aged', '65', 'and', 'over', 'became', 'the', 'biggest', 'among', 'the', 'Japanese', '.']\n",
      "\n",
      "['The', 'bar', 'chart', 'illustrates', 'the', 'difference', 'in', 'the', 'rate', 'of', 'unemployment', 'in', 'two', 'years', '(', '2014', 'and', '2015', ')', 'in', '5', 'regions']\n",
      "\n",
      "['Spring', \"'s\", 'monthes', 'in', 'Yakutsk', 'are', 'freezy', 'too', ',', 'tempreture', 'can', 'be', 'possible', 'not', 'higher', 'then', \"0'C\", '.']\n",
      "\n",
      "['The', 'The', 'same', 'same', 'thing', 'thing', 'happens', 'happens', 'with', 'with', 'buildings', 'buildings', '-', '-', 'for', 'for', 'most', 'most', 'of', 'of', 'the', 'the', 'people', 'people', 'it', 'it', 'would', 'would', 'be', 'be', 'more', 'more', 'comfortable', 'comfortable', 'to', 'to', 'visit', 'visit', 'a', 'a', 'good', 'good', '-', '-', 'looking', 'looking', ',', ',', 'bright', 'bright', '-', '-', 'coloured', 'coloured', 'building', 'building', ',', ',', 'instead', 'instead', 'of', 'of', 'a', 'a', 'dark', 'dark', ',', ',', 'simply', 'simply', 'constructed', 'constructed', 'place', 'place', '.', '.']\n",
      "\n",
      "['The', 'The', 'authors', 'authors', 'of', 'of', 'music', 'music', 'and', 'and', 'film', 'film', 'directors', 'directors', 'spend', 'spend', 'their', 'their', 'working', 'working', 'hours', 'hours', 'to', 'to', 'create', 'create', 'a', 'a', 'new', 'new', 'song', 'song', 'or', 'or', 'to', 'to', 'shoot', 'shoot', 'a', 'a', 'new', 'new', 'film', 'film', '.', '.']\n",
      "\n",
      "['People', 'People', 'who', 'who', 'create', 'create', 'illegal', 'illegal', 'pirate', 'pirate', 'copies', 'copies', 'are', 'are', 'responsible', 'responsible', 'for', 'for', 'this', 'this', 'and', 'and', 'should', 'should', 'be', 'be', 'punished', 'punished', '.', '.']\n",
      "\n",
      "['3,9', ',', 'unemployement', 'rate', '.']\n",
      "\n",
      "['Latin', 'America', 'and', 'other', 'world', 'regions', '(', 'worldwide', ')', 'and', 'there', \"'s\", 'no', 'a', 'lot', 'of', 'difference', 'of', 'their', '%', \"'s\", 'in', '2014', 'and', '2015', '.']\n",
      "\n",
      "['The', 'The', 'coldest', 'coldest', 'period', 'period', 'is', 'is', 'in', 'in', 'winter', 'winter', ',', ',', 'from', 'from', 'November', 'November', 'to', 'to', 'March', 'March', ',', ',', 'with', 'with', 'the', 'the', 'lowest', 'lowest', 'temperature', 'temperature', 'in', 'in', 'January', 'January', 'and', 'and', 'December', 'December', '(', '(', 'about', 'about', '-', '-', '40', '40', 'degrees', 'degrees', ')', ')', '.', '.']\n",
      "\n",
      "['And', 'And', 'it', 'it', 'is', 'is', 'not', 'not', 'good', 'good', ',', ',', 'when', 'when', 'there', 'there', 'are', 'are', 'only', 'only', 'boys', 'boys', 'in', 'in', 'art', 'art', 'history', 'history', 'classes', 'classes', '.', '.']\n",
      "\n",
      "['Technologies', 'Technologies', 'are', 'are', 'an', 'an', 'essential', 'essential', 'part', 'part', 'of', 'of', 'modern', 'modern', 'life', 'life', '.', '.']\n",
      "\n",
      "['Thus', ',', 'for', '7', 'months', 'the', 'temperature', 'jumps', 'up', 'by', 'approximately', '65C', 'from', 'just', 'under', '-', '40', 'to', 'a', 'little', 'higher', 'than', '25C', '.']\n",
      "\n",
      "['How', 'How', 'can', 'can', 'this', 'this', 'problem', 'problem', 'be', 'be', 'solved', 'solved', '?', '?']\n",
      "\n",
      "['The', 'The', 'main', 'main', 'point', 'point', 'is', 'is', 'to', 'to', 'help', 'help', 'every', 'every', 'person', 'person', 'in', 'in', 'need', 'need', 'to', 'to', 'overcome', 'overcome', 'the', 'the', 'struggles', 'struggles', '.', '.']\n",
      "\n",
      "['So', 'So', 'should', 'should', 'modern', 'modern', 'pirates', 'pirates', 'be', 'be', 'punished', 'punished', '?', '?']\n",
      "\n",
      "['Nowadays', 'Nowadays', 'sport', 'sport', 'plays', 'plays', 'the', 'the', 'essential', 'essential', 'role', 'role', 'in', 'in', 'everyday', 'everyday', 'people', 'people', \"'s\", \"'s\", 'life', 'life', '.', '.']\n",
      "\n",
      "['Nevertheless', 'Nevertheless', ',', ',', 'there', 'there', 'are', 'are', 'some', 'some', 'factors', 'factors', 'that', 'that', 'will', 'will', 'help', 'help', 'everybody', 'everybody', 'to', 'to', 'be', 'be', 'happy', 'happy', '.', '.']\n",
      "\n",
      "['The', 'The', 'number', 'number', 'of', 'of', 'people', 'people', 'between', 'between', 'the', 'the', 'ages', 'ages', 'of', 'of', '15', '15', 'and', 'and', '59', '59', 'is', 'is', 'predicted', 'predicted', 'to', 'to', 'increase', 'increase', 'in', 'in', 'case', 'case', 'of', 'of', 'Yemen', 'Yemen', 'and', 'and', 'stay', 'stay', 'at', 'at', '57,3', '57,3', '%', '%', 'in', 'in', '2050', '2050', '.', '.']\n",
      "\n",
      "['Painting', 'Painting', 'or', 'or', 'music', 'music', 'helps', 'helps', 'children', 'children', 'to', 'to', 'decrease', 'decrease', 'their', 'their', 'pain', 'pain', '.', '.']\n",
      "\n",
      "['Nowadays', 'Nowadays', ',', ',', 'there', 'there', 'are', 'are', 'a', 'a', 'lot', 'lot', 'of', 'of', 'opinions', 'opinions', 'about', 'about', 'young', 'young', 'people', 'people', \"'s\", \"'s\", 'work', 'work', '.', '.']\n",
      "\n",
      "['By', 'By', '2050', '2050', 'the', 'the', 'percentage', 'percentage', 'of', 'of', 'people', 'people', 'from', 'from', '0', '0', 'to', 'to', '14', '14', 'years', 'years', 'will', 'will', 'fall', 'fall', 'from', 'from', '50,1', '50,1', '%', '%', 'in', 'in', '2000', '2000', 'to', 'to', '37,0', '37,0', '%', '%', 'in', 'in', '2050', '2050', '.', '.']\n",
      "\n",
      "['We', 'We', 'are', 'are', 'not', 'not', 'only', 'only', 'what', 'what', 'we', 'we', 'were', 'were', 'born', 'born', 'with', 'with', ',', ',', 'but', 'but', 'also', 'also', 'what', 'what', 'we', 'we', 'see', 'see', ',', ',', 'with', 'with', 'whom', 'whom', 'we', 'we', 'talk', 'talk', ',', ',', 'in', 'in', 'what', 'what', 'we', 'we', 'believe', 'believe', '.', '.']\n",
      "\n",
      "['Schools', 'Schools', 'and', 'and', 'universities', 'universities', 'should', 'should', 'shift', 'shift', 'from', 'from', 'pressure', 'pressure', 'and', 'and', 'everlasting', 'everlasting', 'competition', 'competition', 'among', 'among', 'students', 'students', 'to', 'to', 'giving', 'giving', 'them', 'them', 'opportunities', 'opportunities', 'to', 'to', 'explore', 'explore', 'the', 'the', 'world', 'world', 'and', 'and', 'find', 'find', 'themselves', 'themselves', '.', '.']\n",
      "\n",
      "['Overall', 'Overall', ',', ',', 'it', 'it', 'might', 'might', 'be', 'be', 'deduced', 'deduced', 'from', 'from', 'the', 'the', 'illustration', 'illustration', 'that', 'that', 'by', 'by', 'the', 'the', 'end', 'end', 'of', 'of', 'the', 'the', 'period', 'period', 'from', 'from', '1940', '1940', 'to', 'to', '2040', '2040', 'the', 'the', 'proportion', 'proportion', 'of', 'of', 'population', 'population', 'aged', 'aged', '65', '65', 'and', 'and', 'over', 'over', 'increases', 'increases', 'dramatically', 'dramatically', 'in', 'in', 'all', 'all', 'of', 'of', 'the', 'the', 'three', 'three', 'countries', 'countries', '.', '.']\n",
      "\n",
      "['I', 'I', 'think', 'think', 'it', 'it', 'is', 'is', 'wrong', 'wrong', '.', '.']\n",
      "\n",
      "['In', 'In', '1940', '1940', 'there', 'there', 'were', 'were', 'only', 'only', '5', '5', '%', '%', 'of', 'of', 'people', 'people', 'over', 'over', '65', '65', 'in', 'in', 'Japan', 'Japan', '.', '.']\n",
      "\n",
      "['According', 'to', 'the', 'statistic', ',', 'Facebook', 'is', 'the', 'most', 'popular', 'social', 'network', 'in', 'the', 'U.', 'S', '.']\n",
      "\n",
      "['For', 'example', ',', 'Rodion', 'Raskolnikov']\n",
      "\n",
      "['The', 'The', 'most', 'most', 'significant', 'significant', 'difference', 'difference', 'is', 'is', 'between', 'between', 'young', 'young', 'adults', 'adults', '.', '.']\n",
      "\n",
      "['Still', 'Still', ',', ',', 'it', 'it', 'would', 'would', 'be', 'be', 'silly', 'silly', 'to', 'to', 'neglect', 'neglect', 'this', 'this', 'problem', 'problem', '.', '.']\n",
      "\n",
      "['Overall', 'Overall', ',', ',', 'the', 'the', 'oldest', 'oldest', 'and', 'and', 'the', 'the', 'longest', 'longest', 'underground', 'underground', 'is', 'is', 'in', 'in', 'London', 'London', '.', '.']\n",
      "\n",
      "['As', 'we', 'know', ',', 'the', 'fashion', 'inductry', 'takes', 'place', 'in', 'those', 'cities', ';']\n",
      "\n",
      "['They', 'They', 'need', 'need', 'it', 'it', 'so', 'so', 'much', 'much', 'that', 'that', 'they', 'they', 'can', 'can', 'behave', 'behave', 'like', 'like', 'this', 'this', 'to', 'to', 'get', 'get', 'some', 'some', 'attention', 'attention', 'from', 'from', 'their', 'their', 'parents', 'parents', 'a', 'a', 'bit', 'bit', '.', '.']\n",
      "\n",
      "['As', 'As', 'I', 'I', 'think', 'think', ',', ',', 'there', 'there', 'should', 'should', 'be', 'be', 'some', 'some', 'rules', 'rules', 'which', 'which', 'artists', 'artists', 'have', 'have', 'to', 'to', 'keep', 'keep', 'in', 'in', 'mind', 'mind', '.', '.']\n",
      "\n",
      "['The', 'The', 'numbers', 'numbers', 'are', 'are', '12,5', '12,5', 'and', 'and', '12,5', '12,5', '.', '.']\n",
      "\n",
      "['Firstly', 'Firstly', ',', ',', 'kids', 'kids', 'and', 'and', 'youngsters', 'youngsters', 'are', 'are', 'exposed', 'exposed', 'to', 'to', 'violence', 'violence', 'and', 'and', 'criminal', 'criminal', 'behavior', 'behavior', 'from', 'from', 'a', 'a', 'young', 'young', 'age', 'age', '.', '.']\n",
      "\n",
      "['We', 'We', 'should', 'should', 'cope', 'cope', 'with', 'with', 'stress', 'stress', 'permanently', 'permanently', '.', '.']\n",
      "\n",
      "['Firstly', 'Firstly', ',', ',', 'it', 'it', 'is', 'is', 'not', 'not', 'smart', 'smart', 'to', 'to', 'spend', 'spend', 'a', 'a', 'huge', 'huge', 'part', 'part', 'of', 'of', 'country', 'country', \"'s\", \"'s\", 'budget', 'budget', 'on', 'on', 'space', 'space', 'exploration', 'exploration', 'when', 'when', 'the', 'the', 'economic', 'economic', 'situation', 'situation', 'in', 'in', 'your', 'your', 'country', 'country', 'is', 'is', 'not', 'not', 'very', 'very', 'stable', 'stable', '.', '.']\n",
      "\n",
      "['However', 'However', ',', ',', 'there', 'there', 'are', 'are', 'a', 'a', 'lot', 'lot', 'of', 'of', 'women', 'women', 'in', 'in', 'the', 'the', 'sphere', 'sphere', 'of', 'of', 'services', 'services', '.', '.']\n",
      "\n",
      "['There', 'There', 'is', 'is', 'a', 'a', 'serious', 'serious', 'problem', 'problem', 'with', 'with', 'pirates', 'pirates', 'on', 'on', 'the', 'the', 'Internet', 'Internet', '.', '.']\n",
      "\n",
      "['I', 'I', 'think', 'think', 'that', 'that', 'both', 'both', 'points', 'points', 'of', 'of', 'view', 'view', 'expressed', 'expressed', 'in', 'in', 'this', 'this', 'essay', 'essay', 'can', 'can', 'exist', 'exist', 'because', 'because', 'the', 'the', 'border', 'border', 'between', 'between', 'these', 'these', 'two', 'two', 'decisions', 'decisions', 'is', 'is', 'too', 'too', 'subtle', 'subtle', '.', '.']\n",
      "\n",
      "['They', 'They', 'should', 'should', 'realize', 'realize', 'that', 'that', 'there', 'there', 'is', 'is', 'no', 'no', '\"', '\"', 'Save', 'Save', '/', '/', 'Load', 'Load', '\"', '\"', 'option', 'option', 'in', 'in', 'real', 'real', 'life', 'life', 'and', 'and', 'that', 'that', 'they', 'they', 'take', 'take', 'full', 'full', 'responsibility', 'responsibility', 'for', 'for', 'their', 'their', 'unlawful', 'unlawful', 'deeds', 'deeds', 'and', 'and', 'actions', 'actions', '.', '.']\n",
      "\n",
      "['Moreover', 'Moreover', ',', ',', 'such', 'such', 'kind', 'kind', 'of', 'of', 'mixed', 'mixed', 'community', 'community', 'can', 'can', 'distract', 'distract', 'students', 'students', 'from', 'from', 'their', 'their', 'study', 'study', ',', ',', 'while', 'while', 'getting', 'getting', 'knowledge', 'knowledge', 'is', 'is', 'the', 'the', 'most', 'most', 'important', 'important', 'part', 'part', 'of', 'of', 'student', 'student', \"'s\", \"'s\", 'life', 'life', '.', '.']\n",
      "\n",
      "['In', 'In', 'this', 'this', 'case', 'case', ',', ',', 'it', 'it', 'is', 'is', 'up', 'up', 'to', 'to', 'the', 'the', 'government', 'government', 'to', 'to', 'solve', 'solve', 'the', 'the', 'problem', 'problem', 'on', 'on', 'this', 'this', 'level', 'level', '.', '.']\n",
      "\n",
      "['That', 'That', 'clearly', 'clearly', 'shows', 'shows', 'us', 'us', 'that', 'that', 'it', 'it', 'has', 'has', 'become', 'become', 'about', 'about', 'three', 'three', 'times', 'times', 'warmer', 'warmer', 'than', 'than', 'it', 'it', 'used', 'used', 'to', 'to', 'be', 'be', 'in', 'in', 'the', 'the', 'beginning', 'beginning', 'of', 'of', 'this', 'this', 'period', 'period', '.', '.']\n",
      "\n",
      "['And', 'And', 'then', 'then', 'we', 'we', 'can', 'can', 'see', 'see', 'a', 'a', 'noticeable', 'noticeable', 'decrease', 'decrease', 'in', 'in', 'average', 'average', 'temperatures', 'temperatures', '.', '.']\n",
      "\n",
      "['These', 'These', 'are', 'are', 'two', 'two', 'goals', 'goals', 'that', 'that', 'ought', 'ought', 'to', 'to', 'be', 'be', 'achieved', 'achieved', 'to', 'to', 'make', 'make', 'one', 'one', \"'s\", \"'s\", 'life', 'life', 'bright', 'bright', 'and', 'and', 'happy', 'happy', '.', '.']\n",
      "\n",
      "['It', 'It', 'is', 'is', 'difficult', 'difficult', 'and', 'and', 'I', 'I', 'will', 'will', 'try', 'try', 'to', 'to', 'explain', 'explain', 'why', 'why', '.', '.']\n",
      "\n",
      "['The', 'The', 'problem', 'problem', 'of', 'of', 'public', 'public', 'health', 'health', 'is', 'is', 'getting', 'getting', 'more', 'more', 'and', 'and', 'more', 'more', 'important', 'important', 'nowadays', 'nowadays', '.', '.']\n",
      "\n",
      "['The', 'following', 'bar', 'chart', 'illustrates', 'the', 'use', 'of', 'major', 'social', 'networks', ':', 'Facebook', ',', 'Instagram', 'and', 'LinkedIn', 'among', 'U.S', '.', 'adults', 'divided', 'into', '4', 'age', 'groups', '.']\n",
      "\n",
      "['I', 'I', 'see', 'see', 'it', 'it', 'as', 'as', 'a', 'a', 'result', 'result', 'of', 'of', 'lack', 'lack', 'of', 'of', 'morality', 'morality', 'in', 'in', 'our', 'our', 'everyday', 'everyday', 'life', 'life', '.', '.']\n",
      "\n",
      "['This', 'This', 'bar', 'bar', 'chart', 'chart', 'provides', 'provides', 'information', 'information', 'on', 'on', 'the', 'the', 'dynamics', 'dynamics', 'of', 'of', 'unemployment', 'unemployment', 'level', 'level', 'in', 'in', 'five', 'five', 'world', 'world', 'regions', 'regions', 'in', 'in', '2014', '2014', 'and', 'and', '2015', '2015', '.', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab48bf98-ca55-4a0b-9715-aa986bfafe89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
