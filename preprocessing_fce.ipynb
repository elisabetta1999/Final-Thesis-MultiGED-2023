{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7173db-1157-4ee7-833e-5e44bd78bdf2",
   "metadata": {},
   "source": [
    "# FCE Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5e62734-73a1-4568-8b5a-45e69143a786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elisabetta/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "/home/elisabetta/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from itertools import zip_longest\n",
    "import csv\n",
    "import unicodedata\n",
    "import thesis_utils\n",
    "from collections import defaultdict, Counter\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from rapidfuzz.fuzz import ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19a47f8a-a7e2-49c5-a29c-be70c7d1cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fce_dataset = '/home/elisabetta/Scrivania/thesis/fce-released-dataset/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0e4f9cb-f730-4f19-8109-e1a819787b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2191\n"
     ]
    }
   ],
   "source": [
    "multiged_fce_dev =  thesis_utils.read_tsv_file_and_find_sentences_without_headers('./MULTI-GED2023 DATA/en_fce_dev.tsv')\n",
    "multiged_dev_info2 = thesis_utils.get_list_ids_tokens_gold(multiged_fce_dev)\n",
    "print(len(multiged_fce_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9044ac7a-9f65-4e6e-b242-8fb59f4a9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_multiged(sentences_tokens_lists):\n",
    "    for sentence in sentences_tokens_lists:\n",
    "        for i, (idx, token, label) in enumerate(sentence):\n",
    "            if token.startswith('\\\\'):\n",
    "                cleaned_token = token.strip('\\\\')\n",
    "                sentence[i] = (idx, cleaned_token, label)\n",
    "    return sentences_tokens_lists\n",
    "\n",
    "\n",
    "multiged_dev_info = clean_multiged(multiged_dev_info2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49188db3-8233-47bc-89bd-5dea2c316bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('T0', '13th', 'c'), ('T1', 'June', 'c'), ('T2', '2000', 'c')]\n"
     ]
    }
   ],
   "source": [
    "print(multiged_dev_info[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e93dec80-f773-4857-b31b-4c81ba642f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALMOST THERE...\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# def process_fce_directory_return_sentences_spacy2_ins(input_dir):\n",
    "#     all_sentences = []\n",
    "#     token_counter = 0\n",
    "\n",
    "#     for root_dir, _, files in os.walk(input_dir):\n",
    "#         for filename in files:\n",
    "#             if filename.endswith('.xml') and '-checkpoint' not in filename:\n",
    "#                 filepath = os.path.join(root_dir, filename)\n",
    "\n",
    "#                 try:\n",
    "#                     tree = etree.parse(filepath)\n",
    "#                     root = tree.getroot()\n",
    "#                     coded_answers = root.findall('.//coded_answer')\n",
    "\n",
    "#                     for coded in coded_answers:\n",
    "#                         for p in coded.findall('.//p'):\n",
    "#                             current_sentence = []\n",
    "\n",
    "#                             def process_text(text, label, correction='', error_type='', is_correction=False):\n",
    "#                                 nonlocal current_sentence, token_counter\n",
    "#                                 if not text or not text.strip():\n",
    "#                                     return\n",
    "\n",
    "#                                 doc = nlp(text.strip())\n",
    "#                                 for token in doc:\n",
    "#                                     token_counter += 1\n",
    "#                                     row = [\n",
    "#                                         filename,\n",
    "#                                         f\"T{token_counter}\",      # Only global token counter\n",
    "#                                         token.text,\n",
    "#                                         label,\n",
    "#                                         correction,\n",
    "#                                         error_type if label == 'i' else ''\n",
    "#                                     ]\n",
    "#                                     current_sentence.append(row)\n",
    "                                    \n",
    "#                                     if token.text in ['.', '!', '?']:\n",
    "#                                         all_sentences.append(current_sentence)\n",
    "#                                         current_sentence = []\n",
    "\n",
    "#                             def process_element(elem, label='c', correction='', error_type=''):\n",
    "#                                 nonlocal current_sentence\n",
    "\n",
    "#                                 if elem.text:\n",
    "#                                     if label != 'c' or not isinstance(elem.tag, str) or elem.tag != 'c':\n",
    "#                                         process_text(elem.text, label, correction, error_type)\n",
    "\n",
    "#                                 for child in elem:\n",
    "#                                     if child.tag == 'NS':\n",
    "#                                         etype = child.get('type', '')\n",
    "#                                         i_elem = child.find('i')\n",
    "#                                         c_elem = child.find('c')\n",
    "\n",
    "#                                         if etype.startswith('M') and c_elem is not None:\n",
    "#                                             correction_text = c_elem.text.strip() if c_elem.text else ''\n",
    "#                                             process_text(\n",
    "#                                                 '',\n",
    "#                                                 'c',  # Mark as correct\n",
    "#                                                 correction=correction_text,\n",
    "#                                                 error_type=etype\n",
    "#                                             )\n",
    "\n",
    "#                                         if i_elem is not None:\n",
    "#                                             process_element(\n",
    "#                                                 i_elem,\n",
    "#                                                 label='i',\n",
    "#                                                 correction=c_elem.text if c_elem is not None else '',\n",
    "#                                                 error_type=etype\n",
    "#                                             )\n",
    "\n",
    "#                                     elif child.tag == 'i':\n",
    "#                                         process_element(child, label='i', correction=correction, error_type=error_type)\n",
    "#                                     elif child.tag == 'c':\n",
    "#                                         pass\n",
    "#                                     else:\n",
    "#                                         process_element(child, label=label, correction=correction, error_type=error_type)\n",
    "\n",
    "#                                     if child.tail:\n",
    "#                                         process_text(child.tail, 'c')\n",
    "\n",
    "#                             process_element(p)\n",
    "\n",
    "#                             if current_sentence:\n",
    "#                                 all_sentences.append(current_sentence)\n",
    "#                                 current_sentence = []\n",
    "\n",
    "#                 except etree.XMLSyntaxError as e:\n",
    "#                     print(f\"XML parse error in {filename}: {e}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "#     return all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c33a656-d9f9-4394-9d03-55edabd5f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERFETTO!!!\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "from lxml import etree\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def process_fce_directory_return_sentences_spacy2_ins2(input_dir):\n",
    "    all_sentences = []\n",
    "    token_counter = 0\n",
    "\n",
    "    for root_dir, _, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.xml') and '-checkpoint' not in filename:\n",
    "                filepath = os.path.join(root_dir, filename)\n",
    "\n",
    "                try:\n",
    "                    tree = etree.parse(filepath)\n",
    "                    root = tree.getroot()\n",
    "                    coded_answers = root.findall('.//coded_answer')\n",
    "\n",
    "                    for coded in coded_answers:\n",
    "                        for p in coded.findall('.//p'):\n",
    "                            current_sentence = [] #element\n",
    "\n",
    "                            def process_text(text, label, correction='', error_type='', is_correction=False):\n",
    "                                nonlocal current_sentence, token_counter\n",
    "                                if not text or not text.strip():\n",
    "                                    return\n",
    "\n",
    "                                doc = nlp(text.strip())\n",
    "                                for token in doc:\n",
    "                                    token_counter += 1\n",
    "                                    row = [\n",
    "                                        filename,\n",
    "                                        f\"T{token_counter}\",      # Only global token counter\n",
    "                                        token.text,\n",
    "                                        label,\n",
    "                                        correction,\n",
    "                                        error_type if label == 'i' else ''\n",
    "                                    ]\n",
    "                                    current_sentence.append(row)\n",
    "                                    \n",
    "                                    if token.text in ['.', '!', '?']:\n",
    "                                        all_sentences.append(current_sentence)\n",
    "                                        current_sentence = []\n",
    "\n",
    "                            def process_element(elem, label='c', correction='', error_type=''):\n",
    "                                nonlocal current_sentence\n",
    "\n",
    "                                if elem.text:\n",
    "                                    if label != 'c' or not isinstance(elem.tag, str) or elem.tag != 'c':\n",
    "                                        process_text(elem.text, label, correction, error_type)\n",
    "\n",
    "                                for child in elem:\n",
    "                                    if child.tag == 'NS':\n",
    "                                        etype = child.get('type', '')\n",
    "                                        i_elem = child.find('i')\n",
    "                                        c_elem = child.find('c')\n",
    "\n",
    "                                        # Handle Metalinguistic cases with explicit <c>\n",
    "                                        if etype.startswith('M') and c_elem is not None:\n",
    "                                            correction_text = c_elem.text.strip() if c_elem.text else ''\n",
    "                                            process_text(\n",
    "                                                '',\n",
    "                                                'c',  # Mark as correct\n",
    "                                                correction=correction_text,\n",
    "                                                error_type=etype\n",
    "                                            )\n",
    "\n",
    "                                        # Handle standard <i> cases\n",
    "                                        if i_elem is not None:\n",
    "                                            process_element(\n",
    "                                                i_elem,\n",
    "                                                label='i',\n",
    "                                                correction=c_elem.text if c_elem is not None else '',\n",
    "                                                error_type=etype\n",
    "                                            )\n",
    "\n",
    "                                        # Handle R-type errors with no <i> or <c>\n",
    "                                        elif not list(child):  # No children: assume plain incorrect span\n",
    "                                            process_text(\n",
    "                                                child.text,\n",
    "                                                label='i',\n",
    "                                                correction='',\n",
    "                                                error_type=etype\n",
    "                                            )\n",
    "\n",
    "                                    elif child.tag == 'i':\n",
    "                                        process_element(child, label='i', correction=correction, error_type=error_type)\n",
    "                                    elif child.tag == 'c':\n",
    "                                        pass  # Do not process <c> directly unless in M-type handling\n",
    "                                    else:\n",
    "                                        process_element(child, label=label, correction=correction, error_type=error_type)\n",
    "\n",
    "                                    if child.tail:\n",
    "                                        process_text(child.tail, 'c')\n",
    "\n",
    "                            process_element(p)\n",
    "\n",
    "                            if current_sentence:\n",
    "                                all_sentences.append(current_sentence)\n",
    "                                current_sentence = []\n",
    "\n",
    "                except etree.XMLSyntaxError as e:\n",
    "                    print(f\"XML parse error in {filename}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    return all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64e0f2d6-4a5b-4222-a980-4f4b472b92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import spacy\n",
    "# from lxml import etree\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# def process_fce_directory_return_sentences_spacy2_ins4(input_dir):\n",
    "#     all_sentences = []\n",
    "#     token_counter = 0\n",
    "\n",
    "#     for root_dir, _, files in os.walk(input_dir):\n",
    "#         for filename in files:\n",
    "#             if filename.endswith('.xml') and '-checkpoint' not in filename:\n",
    "#                 filepath = os.path.join(root_dir, filename)\n",
    "\n",
    "#                 try:\n",
    "#                     tree = etree.parse(filepath)\n",
    "#                     root = tree.getroot()\n",
    "#                     coded_answers = root.findall('.//coded_answer')\n",
    "\n",
    "#                     for coded in coded_answers:\n",
    "#                         for p in coded.findall('.//p'):\n",
    "#                             current_sentence = []\n",
    "\n",
    "#                             def process_text(text, label, correction='', error_type='', is_correction=False):\n",
    "#                                 nonlocal current_sentence, token_counter\n",
    "#                                 if not text or not text.strip():\n",
    "#                                     return\n",
    "\n",
    "#                                 doc = nlp(text.strip())\n",
    "#                                 for token in doc:\n",
    "#                                     token_counter += 1\n",
    "#                                     row = [\n",
    "#                                         filename,\n",
    "#                                         f\"T{token_counter}\",      # Only global token counter\n",
    "#                                         token.text,\n",
    "#                                         label,\n",
    "#                                         correction,\n",
    "#                                         error_type if label == 'i' else ''\n",
    "#                                     ]\n",
    "#                                     current_sentence.append(row)\n",
    "\n",
    "#                                     if token.text in ['.', '!', '?']:\n",
    "#                                         all_sentences.append(current_sentence)\n",
    "#                                         current_sentence = []\n",
    "\n",
    "#                             def process_element(elem, label='c', correction='', error_type=''):\n",
    "#                                 nonlocal current_sentence\n",
    "\n",
    "#                                 if elem.text:\n",
    "#                                     if label != 'c' or not isinstance(elem.tag, str) or elem.tag != 'c':\n",
    "#                                         process_text(elem.text, label, correction, error_type)\n",
    "\n",
    "#                                 for child in elem:\n",
    "#                                     if child.tag == 'NS':\n",
    "#                                         etype = child.get('type', '')\n",
    "#                                         i_elem = child.find('i')\n",
    "#                                         c_elem = child.find('c')\n",
    "\n",
    "#                                         # Handle M-type corrections with <c> only\n",
    "#                                         if etype.startswith('M') and c_elem is not None:\n",
    "#                                             correction_text = c_elem.text.strip() if c_elem.text else ''\n",
    "#                                             process_text(\n",
    "#                                                 '',\n",
    "#                                                 'c',\n",
    "#                                                 correction=correction_text,\n",
    "#                                                 error_type=etype\n",
    "#                                             )\n",
    "\n",
    "#                                         # Handle standard i+c structure\n",
    "#                                         if i_elem is not None:\n",
    "#                                             process_element(\n",
    "#                                                 i_elem,\n",
    "#                                                 label='i',\n",
    "#                                                 correction=c_elem.text if c_elem is not None else '',\n",
    "#                                                 error_type=etype\n",
    "#                                             )\n",
    "\n",
    "#                                         # Handle R-type with no children (<NS>just_text</NS>)\n",
    "#                                         elif etype.startswith('R') and not list(child):\n",
    "#                                             process_text(\n",
    "#                                                 child.text,\n",
    "#                                                 label='i',\n",
    "#                                                 correction='',\n",
    "#                                                 error_type=etype\n",
    "#                                             )\n",
    "\n",
    "#                                         else:\n",
    "#                                             # Recurse into nested <NS> tags (like FN inside RN)\n",
    "#                                             for nested in child:\n",
    "#                                                 process_element(\n",
    "#                                                     nested,\n",
    "#                                                     label=label,\n",
    "#                                                     correction=correction,\n",
    "#                                                     error_type=etype\n",
    "#                                                 )\n",
    "\n",
    "#                                     elif child.tag == 'i':\n",
    "#                                         process_element(child, label='i', correction=correction, error_type=error_type)\n",
    "#                                     elif child.tag == 'c':\n",
    "#                                         pass  # Skip <c> unless handled above\n",
    "#                                     else:\n",
    "#                                         process_element(child, label=label, correction=correction, error_type=error_type)\n",
    "\n",
    "#                                     if child.tail:\n",
    "#                                         process_text(child.tail, 'c')\n",
    "\n",
    "#                             process_element(p)\n",
    "\n",
    "#                             if current_sentence:\n",
    "#                                 all_sentences.append(current_sentence)\n",
    "#                                 current_sentence = []\n",
    "\n",
    "#                 except etree.XMLSyntaxError as e:\n",
    "#                     print(f\"XML parse error in {filename}: {e}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "#     return all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e07756f-50fe-4bed-a7d1-5eee26840c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_fce_spacy= process_fce_directory_return_sentences_spacy2_ins2(fce_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c867bc15-0264-41db-8f26-16be23701e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['doc450.xml', 'T148778', 'She', 'c', '', ''], ['doc450.xml', 'T148779', 'has', 'c', '', ''], ['doc450.xml', 'T148780', 'not', 'c', '', ''], ['doc450.xml', 'T148781', 'a', 'c', '', ''], ['doc450.xml', 'T148782', 'lot', 'c', '', ''], ['doc450.xml', 'T148783', 'of', 'c', '', ''], ['doc450.xml', 'T148784', 'friend', 'i', 'friends', 'AGN'], ['doc450.xml', 'T148785', 'because', 'c', '', ''], ['doc450.xml', 'T148786', 'of', 'c', '', ''], ['doc450.xml', 'T148787', 'this', 'c', '', ''], ['doc450.xml', 'T148788', 'point', 'i', '', 'RN'], ['doc450.xml', 'T148789', '.', 'c', '', '']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_fce_spacy[9170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df8040ad-7dd0-40a0-8ef7-fcacab293a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_empty_spaces_sentences_fce_spacy = []\n",
    "\n",
    "for sentence in sentences_fce_spacy:\n",
    "    cleaned_sentence = []\n",
    "\n",
    "    for line in sentence:\n",
    "        token = line[2]  \n",
    "\n",
    "        if token.strip() == '':\n",
    "            continue \n",
    "        cleaned_sentence.append(line)  \n",
    "\n",
    "    if cleaned_sentence:\n",
    "        no_empty_spaces_sentences_fce_spacy.append(cleaned_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f37a8351-d3d8-4794-8427-f6cd6b813d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34415\n"
     ]
    }
   ],
   "source": [
    "print(len(no_empty_spaces_sentences_fce_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0048c298-99e7-4712-83b1-b84fbeed2ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['doc3139.xml', 'T63', 'Our', 'c', '', ''], ['doc3139.xml', 'T64', 'English', 'c', '', ''], ['doc3139.xml', 'T65', 'club', 'c', '', ''], ['doc3139.xml', 'T66', 'is', 'c', '', ''], ['doc3139.xml', 'T67', 'not', 'c', '', ''], ['doc3139.xml', 'T68', 'a', 'c', '', ''], ['doc3139.xml', 'T69', 'very', 'c', '', ''], ['doc3139.xml', 'T70', 'big', 'c', '', ''], ['doc3139.xml', 'T71', 'one', 'c', '', ''], ['doc3139.xml', 'T72', ',', 'c', '', ''], ['doc3139.xml', 'T73', 'with', 'c', '', ''], ['doc3139.xml', 'T74', '14', 'c', '', ''], ['doc3139.xml', 'T75', 'students', 'c', '', ''], ['doc3139.xml', 'T76', 'and', 'c', '', ''], ['doc3139.xml', 'T77', 'a', 'c', '', ''], ['doc3139.xml', 'T78', 'teacher', 'c', '', ''], ['doc3139.xml', 'T79', '.', 'c', '', '']]\n"
     ]
    }
   ],
   "source": [
    "print(no_empty_spaces_sentences_fce_spacy[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a274918e-a184-4243-8c38-5e337b3dda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_tokens_and_renumber(sentences):\n",
    "#     processed_sentences = []\n",
    "#     token_counter = 0  # Global token counter\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         new_sentence = []\n",
    "\n",
    "#         for row in sentence:\n",
    "#             file_name = row[0]\n",
    "#             token_text = row[2]\n",
    "#             other_fields = row[3:]\n",
    "\n",
    "#             if '/' in token_text and any(c.isalpha() for c in token_text):\n",
    "#                 parts = []\n",
    "#                 current = ''\n",
    "#                 for char in token_text:\n",
    "#                     if char == '/':\n",
    "#                         if current:\n",
    "#                             parts.append(current)\n",
    "#                             current = ''\n",
    "#                         parts.append('/')\n",
    "#                     else:\n",
    "#                         current += char\n",
    "#                 if current:\n",
    "#                     parts.append(current)\n",
    "\n",
    "#                 for part in parts:\n",
    "#                     new_row = [\n",
    "#                         file_name,\n",
    "#                         f'T{token_counter}',\n",
    "#                         part,\n",
    "#                         *other_fields\n",
    "#                     ]\n",
    "#                     new_sentence.append(new_row)\n",
    "#                     token_counter += 1\n",
    "\n",
    "#             else:\n",
    "#                 new_row = [\n",
    "#                     file_name,\n",
    "#                     f'T{token_counter}',\n",
    "#                     token_text,\n",
    "#                     *other_fields\n",
    "#                 ]\n",
    "#                 new_sentence.append(new_row)\n",
    "#                 token_counter += 1\n",
    "\n",
    "#         processed_sentences.append(new_sentence)\n",
    "\n",
    "#     return processed_sentences\n",
    "# sent_sp2 = split_tokens_and_renumber(no_empty_spaces_sentences_fce_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5c0152d-dbce-47e8-a362-f282c2a6a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_and_renumber(sentences):\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        new_sentence = []\n",
    "        token_counter = 0  # Reset for each sentence\n",
    "\n",
    "        for row in sentence:\n",
    "            file_name = row[0]\n",
    "            token_text = row[2]\n",
    "            other_fields = row[3:]\n",
    "\n",
    "            if '/' in token_text and any(c.isalpha() for c in token_text):\n",
    "                parts = []\n",
    "                current = ''\n",
    "                for char in token_text:\n",
    "                    if char == '/':\n",
    "                        if current:\n",
    "                            parts.append(current)\n",
    "                            current = ''\n",
    "                        parts.append('/')\n",
    "                    else:\n",
    "                        current += char\n",
    "                if current:\n",
    "                    parts.append(current)\n",
    "\n",
    "                for part in parts:\n",
    "                    new_row = [\n",
    "                        file_name,\n",
    "                        f'T{token_counter}',\n",
    "                        part,\n",
    "                        *other_fields\n",
    "                    ]\n",
    "                    new_sentence.append(new_row)\n",
    "                    token_counter += 1\n",
    "\n",
    "            else:\n",
    "                new_row = [\n",
    "                    file_name,\n",
    "                    f'T{token_counter}',\n",
    "                    token_text,\n",
    "                    *other_fields\n",
    "                ]\n",
    "                new_sentence.append(new_row)\n",
    "                token_counter += 1\n",
    "\n",
    "        processed_sentences.append(new_sentence)\n",
    "\n",
    "    return processed_sentences\n",
    "sent_sp2 = split_tokens_and_renumber(no_empty_spaces_sentences_fce_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "738cd34e-71b7-4db2-a893-7794d40f8a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['doc3139.xml', 'T0', 'My', 'c', '', ''], ['doc3139.xml', 'T1', 'name', 'c', '', ''], ['doc3139.xml', 'T2', 'is', 'c', '', ''], ['doc3139.xml', 'T3', 'Akihiro', 'c', '', ''], ['doc3139.xml', 'T4', 'and', 'c', '', ''], ['doc3139.xml', 'T5', 'I', 'c', '', ''], ['doc3139.xml', 'T6', \"'m\", 'c', '', ''], ['doc3139.xml', 'T7', 'a', 'c', '', ''], ['doc3139.xml', 'T8', 'member', 'c', '', ''], ['doc3139.xml', 'T9', 'of', 'c', '', ''], ['doc3139.xml', 'T10', 'an', 'c', '', ''], ['doc3139.xml', 'T11', 'English', 'c', '', ''], ['doc3139.xml', 'T12', 'club', 'c', '', ''], ['doc3139.xml', 'T13', 'at', 'c', '', ''], ['doc3139.xml', 'T14', 'West', 'c', '', ''], ['doc3139.xml', 'T15', 'Isle', 'c', '', ''], ['doc3139.xml', 'T16', 'College', 'c', '', ''], ['doc3139.xml', 'T17', '.', 'c', '', '']]\n"
     ]
    }
   ],
   "source": [
    "print(sent_sp2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "628cc7c5-0826-4118-9f1b-8150b41973fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weird_merged_sentences = []\n",
    "i = 0\n",
    "n = len(sent_sp2)\n",
    "\n",
    "while i < n:\n",
    "    sentence = sent_sp2[i]\n",
    "\n",
    "    if (\n",
    "        len(sentence) == 2\n",
    "        and sentence[0][2].isdigit()        \n",
    "        and sentence[1][2] in {'.', ')'} \n",
    "    ):\n",
    "        if i + 1 < n:\n",
    "            next_sentence = sent_sp2[i + 1]\n",
    "\n",
    "            weird_merged_sentences.append(sentence)\n",
    "            weird_merged_sentences.append(next_sentence)\n",
    "\n",
    "            merged = sentence + next_sentence\n",
    "            weird_merged_sentences.append(merged)\n",
    "\n",
    "            i += 2\n",
    "        else:\n",
    "            weird_merged_sentences.append(sentence)\n",
    "            i += 1\n",
    "    else:\n",
    "        if sentence[0][2] == '\"':\n",
    "            merged_sentence = previous_sentence + sentence\n",
    "            original_previous_sentence = previous_sentence\n",
    "            original_current_sentence = sentence\n",
    "            stripped_current_sentence = [token for token in sentence if token[2] != '\"']\n",
    "\n",
    "            weird_merged_sentences.append(original_previous_sentence) \n",
    "            weird_merged_sentences.append(original_current_sentence)\n",
    "            weird_merged_sentences.append(merged_sentence)\n",
    "            weird_merged_sentences.append(stripped_current_sentence)\n",
    "        else:\n",
    "            weird_merged_sentences.append(sentence)\n",
    "\n",
    "        i += 1\n",
    "    previous_sentence = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed57686b-e910-4236-994e-534642014349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35006\n"
     ]
    }
   ],
   "source": [
    "print(len(weird_merged_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f95e425-9a17-419f-80a9-b9343e62c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34921\n"
     ]
    }
   ],
   "source": [
    "filtered_sentences_spacy = [\n",
    "    sent for sent in weird_merged_sentences\n",
    "    if not (len(sent) == 1 and sent[0][2] == '.')\n",
    "]\n",
    "print(len(filtered_sentences_spacy))\n",
    "even_more_filtered____ = []\n",
    "for sent in filtered_sentences_spacy:\n",
    "    new_sent = [] \n",
    "    for i, row in enumerate(sent):\n",
    "        if i == 0 or row[3] != sent[i-1][5]:\n",
    "            new_sent.append(row)\n",
    "    even_more_filtered____.append(new_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4effcb24-98e9-402e-bde4-b3d4fc5c31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## take into account '. .'\n",
    "# punctuation_to_merge = set(string.punctuation) - {\".\"}\n",
    "\n",
    "# merged_sentences_spacy_idk = []\n",
    "\n",
    "# for sent in even_more_filtered____:\n",
    "#     if len(sent) == 1:\n",
    "#         token = sent[0][2]\n",
    "\n",
    "#         if token == \"..\":\n",
    "#             if merged_sentences_spacy_idk and merged_sentences_spacy_idk[-1]:\n",
    "#                 merged_sentences_spacy_idk[-1][-1][2] = \"...\"\n",
    "#         elif token in punctuation_to_merge:\n",
    "#             if merged_sentences_spacy_idk:\n",
    "#                 merged_sentences_spacy_idk[-1].extend(sent)\n",
    "#         else:\n",
    "#             merged_sentences_spacy_idk.append(sent)\n",
    "\n",
    "#     elif len(sent) == 2:\n",
    "#         token1 = sent[0][2]\n",
    "#         token2 = sent[1][2]\n",
    "\n",
    "#         if token1 == \".\" and token2 == \".\":\n",
    "#             if merged_sentences_spacy_idk and merged_sentences_spacy_idk[-1]:\n",
    "#                 merged_sentences_spacy_idk[-1][-1][2] = \"...\"\n",
    "#         elif token1 in string.punctuation and token2 in string.punctuation:\n",
    "#             if merged_sentences_spacy_idk:\n",
    "#                 merged_sentences_spacy_idk[-1].extend(sent)\n",
    "#         else:\n",
    "#             merged_sentences_spacy_idk.append(sent)\n",
    "\n",
    "#     else:\n",
    "#         merged_sentences_spacy_idk.append(sent)\n",
    "        \n",
    "# print(len(merged_sentences_spacy_idk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a67af47d-13de-40ea-a94a-897ffcf7ff8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "punctuation_to_merge = set(string.punctuation) - {\".\"}\n",
    "\n",
    "merged_sentences_spacy_idk = []\n",
    "\n",
    "for sent in even_more_filtered____:\n",
    "    if len(sent) == 1:\n",
    "        token = sent[0][2]\n",
    "\n",
    "        if token == \"..\":\n",
    "            if merged_sentences_spacy_idk and merged_sentences_spacy_idk[-1]:\n",
    "                merged_sentences_spacy_idk[-1][-1][2] = \"...\"\n",
    "        elif token in punctuation_to_merge:\n",
    "            if merged_sentences_spacy_idk:\n",
    "                merged_sentences_spacy_idk[-1].extend(sent)\n",
    "        else:\n",
    "            merged_sentences_spacy_idk.append(sent)\n",
    "\n",
    "    elif len(sent) == 2:\n",
    "        token1 = sent[0][2]\n",
    "        token2 = sent[1][2]\n",
    "\n",
    "        if token1 == \".\" and token2 == \".\":\n",
    "            if merged_sentences_spacy_idk and merged_sentences_spacy_idk[-1]:\n",
    "                merged_sentences_spacy_idk[-1][-1][2] = \"...\"\n",
    "        elif token1 in string.punctuation and token2 in string.punctuation:\n",
    "            if merged_sentences_spacy_idk:\n",
    "                merged_sentences_spacy_idk[-1].extend(sent)\n",
    "        else:\n",
    "            merged_sentences_spacy_idk.append(sent)\n",
    "\n",
    "    else:\n",
    "        new_sent = []\n",
    "        skip_next = False\n",
    "        merged_occurred = False\n",
    "\n",
    "        for i in range(len(sent)):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                continue\n",
    "\n",
    "            current_token = sent[i][2]\n",
    "            if i + 1 < len(sent):\n",
    "                next_token = sent[i + 1][2]\n",
    "                if current_token in {\"have\", \"has\"} and next_token == \"n't\":\n",
    "                    # Merge tokens\n",
    "                    merged_token = current_token + next_token\n",
    "                    merged_row = sent[i][:] \n",
    "                    merged_row[2] = merged_token \n",
    "                    new_sent.append(merged_row)\n",
    "                    skip_next = True\n",
    "                    merged_occurred = True\n",
    "                    continue\n",
    "\n",
    "            new_sent.append(sent[i])\n",
    "\n",
    "        merged_sentences_spacy_idk.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6f9d364-7a9c-428d-87ac-7db97e8cf164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34713\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_sentences_spacy_idk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c02a2e8-49af-4ab4-b370-fc8891256be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68793\n"
     ]
    }
   ],
   "source": [
    "combined_sentences_2or3 = []\n",
    "while i < n:\n",
    "    sentence = merged_sentences_spacy_idk[i]\n",
    "    combined_sentences_2or3.append(sentence)\n",
    "\n",
    "    if i + 1 < n:\n",
    "        merged_2 = sentence + merged_sentences_spacy_idk[i + 1]\n",
    "        combined_sentences_2or3.append(merged_2)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "print(len(combined_sentences_2or3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "591c5f85-121a-4543-b525-b23497f95edf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sentence in combined_sentences_2or3:\n",
    "    for line in sentence:\n",
    "        token = line[2]\n",
    "        if token.strip() == '':\n",
    "            print(\"Sentence contains a newline or empty token:\")\n",
    "            print(sentence)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afdaaddb-5d8b-416c-90d4-3da7767eebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_fce_to_multiged(sentences_fce, multiged_tokens):\n",
    "    token_sequence_to_fce_occurrences = defaultdict(list)\n",
    "    for fce_sentence in sentences_fce:\n",
    "        token_sequence = tuple(token_info[2] for token_info in fce_sentence)\n",
    "        token_sequence_to_fce_occurrences[token_sequence].append(fce_sentence)\n",
    "\n",
    "    token_sequence_to_multiged_occurrences = defaultdict(list)\n",
    "    for multiged_sent in multiged_tokens:\n",
    "        token_sequence = tuple(token[1] for token in multiged_sent)\n",
    "        token_sequence_to_multiged_occurrences[token_sequence].append(multiged_sent)\n",
    "\n",
    "    matched_sentences = []\n",
    "    matched_sequences = set()\n",
    "\n",
    "    for token_sequence, fce_occurrences in token_sequence_to_fce_occurrences.items():\n",
    "        multiged_occurrences = token_sequence_to_multiged_occurrences.get(token_sequence, [])\n",
    "        match_count = min(len(fce_occurrences), len(multiged_occurrences))\n",
    "\n",
    "        for i in range(match_count):\n",
    "            fce_sentence = fce_occurrences[i]\n",
    "            multiged_sentence = multiged_occurrences[i]\n",
    "\n",
    "            if len(fce_sentence) != len(multiged_sentence):\n",
    "                continue\n",
    "\n",
    "            fce_with_labels = [\n",
    "                tuple(token_info) + (multiged_label,)\n",
    "                for token_info, (_, _, multiged_label) in zip(fce_sentence, multiged_sentence)\n",
    "            ]\n",
    "\n",
    "            matched_sentences.append(fce_with_labels)\n",
    "            matched_sequences.add(token_sequence)\n",
    "\n",
    "    return matched_sentences, matched_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea4eed55-9801-4574-914d-a42f06c01cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matched dev sentences: 2149\n",
      "Dev sentences still to match: 42\n",
      "Total unmatched FCE sentences: 64379\n"
     ]
    }
   ],
   "source": [
    "dev_matches_spacy_idk, dev_sequences_spacy_idk = match_fce_to_multiged(combined_sentences_2or3, multiged_dev_info)\n",
    "\n",
    "print(f'Number of matched dev sentences: {len(dev_matches_spacy_idk)}')       \n",
    "\n",
    "all_matched_sequences_spacy = dev_sequences_spacy_idk\n",
    "\n",
    "unmatched_multiged_dev = []\n",
    "for sentence in multiged_dev_info:\n",
    "    token_sequence = tuple(token[1] for token in sentence)\n",
    "    if token_sequence not in all_matched_sequences_spacy:\n",
    "        unmatched_multiged_dev.append(sentence)\n",
    "\n",
    "print(f'Dev sentences still to match: {len(unmatched_multiged_dev)}')\n",
    "\n",
    "unmatched_sentences_fce = []\n",
    "for fce_sentence in combined_sentences_2or3:\n",
    "    token_sequence = tuple(token_info[2] for token_info in fce_sentence)\n",
    "    if token_sequence not in all_matched_sequences_spacy:\n",
    "        unmatched_sentences_fce.append(fce_sentence)\n",
    "\n",
    "print(f\"Total unmatched FCE sentences: {len(unmatched_sentences_fce)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "595e3f19-cdf8-4f7e-9b34-8efeffce1c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_sentence_counts(data, index):\n",
    "    return Counter(tuple(token[index] for token in sentence) for sentence in data)\n",
    "\n",
    "def compare_sentence_counts(data_matches, data_dev):\n",
    "    counts_matches = extract_sentence_counts(data_matches, 2)\n",
    "    counts_dev = extract_sentence_counts(data_dev, 1)\n",
    "\n",
    "    all_sentences = set(counts_matches.keys()) | set(counts_dev.keys())\n",
    "\n",
    "    sorted_sentences = sorted(all_sentences)\n",
    "\n",
    "    print(f\"{'Sentence':<60} | {'Matches':<8} | {'Dev':<8}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for sent in sorted_sentences:\n",
    "        matches_count = counts_matches.get(sent, 0)\n",
    "        dev_count = counts_dev.get(sent, 0)\n",
    "        if matches_count > 1 or dev_count > 1:\n",
    "            sent_str = ' '.join(sent)\n",
    "            print(f\"{sent_str:<60} | {matches_count:<8} | {dev_count:<8}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80083ae0-d4d8-40ea-9969-5416cc0ea652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence                                                     | Matches  | Dev     \n",
      "-------------------------------------------------------------------------------------\n",
      "13th June , 2000                                             | 4        | 4       \n",
      "13th June 2000                                               | 4        | 4       \n",
      "Best wishes                                                  | 3        | 3       \n",
      "Competition Organiser                                        | 3        | 3       \n",
      "Dear Helen ,                                                 | 2        | 2       \n",
      "Dear Kim                                                     | 3        | 3       \n",
      "Dear Kim ,                                                   | 11       | 11      \n",
      "Dear Kim :                                                   | 2        | 2       \n",
      "Dear Mr Robertson ,                                          | 4        | 4       \n",
      "Dear Mrs Ryan ,                                              | 4        | 4       \n",
      "Dear Ms Helen Ryan                                           | 2        | 2       \n",
      "Dear Ms Ryan                                                 | 2        | 2       \n",
      "Dear Ms Ryan ,                                               | 3        | 3       \n",
      "Dear Sir ,                                                   | 4        | 4       \n",
      "Dear Sir / Madam ,                                           | 7        | 7       \n",
      "Dear Sir / Madam :                                           | 2        | 2       \n",
      "Dear Sir or Madam ,                                          | 6        | 6       \n",
      "Home sweet home .                                            | 2        | 2       \n",
      "How are you ?                                                | 4        | 4       \n",
      "I 'm looking forward to hearing from you .                   | 3        | 3       \n",
      "I 'm looking forward to hearing from you soon .              | 4        | 4       \n",
      "I am looking forward to hearing from you .                   | 3        | 3       \n",
      "I look forward to hearing from you .                         | 10       | 10      \n",
      "I look forward to hearing from you soon .                    | 2        | 2       \n",
      "Looking forward to hearing from you .                        | 2        | 2       \n",
      "Love ,                                                       | 3        | 3       \n",
      "Manela B.                                                    | 2        | 2       \n",
      "Shopping is not always enjoyable                             | 3        | 3       \n",
      "Unfortunately , Pat was n't very good at keeping secrets .   | 9        | 9       \n",
      "Your Sincerely                                               | 2        | 2       \n",
      "Your faithfully ,                                            | 3        | 3       \n",
      "Yours faithfully                                             | 10       | 10      \n",
      "Yours faithfully ,                                           | 15       | 15      \n",
      "Yours sincerely                                              | 11       | 11      \n",
      "Yours sincerely ,                                            | 16       | 16      \n"
     ]
    }
   ],
   "source": [
    "compare_sentence_counts(dev_matches_spacy_idk, multiged_dev_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8501f74-551e-4a18-812c-483319d72a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# headers = ['filename', 'token', 'correction', 'error_type', 'gold_label']\n",
    "\n",
    "# with open('processed_fce2.tsv', 'w', newline='', encoding='utf-8') as f_out:\n",
    "#     writer = csv.writer(f_out, delimiter='\\t')\n",
    "#     writer.writerow(headers)\n",
    "\n",
    "#     for sentence in dev_matches_spacy_idk:\n",
    "#         for row in sentence:\n",
    "#             row_without_id = [row[0], row[2], row[4], row[5], row[6]]\n",
    "#             writer.writerow(row_without_id)\n",
    "#         writer.writerow([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86f6e26b-9cce-4b0e-9c37-f7a7baebbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting extra sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "431cb266-2ff5-4341-b5de-fe82e61cd2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_idx_and_filenames(list_of_lines_for_sentence):\n",
    "    all_sentences_tok = []\n",
    "    all_indexes= []\n",
    "    all_filenames = []\n",
    "    \n",
    "    for sent in list_of_lines_for_sentence:\n",
    "        sent_tok = []\n",
    "        sent_idx = []\n",
    "        sent_nam = []\n",
    "        for line in sent:\n",
    "            file_name = line[0]\n",
    "            tok_idx=line[1]\n",
    "            token = line[2]\n",
    "            sent_tok.append(token)\n",
    "            sent_idx.append(tok_idx)\n",
    "            sent_nam.append(file_name)\n",
    "        all_sentences_tok.append(sent_tok)\n",
    "        all_indexes.append(sent_idx)\n",
    "        all_filenames.append(sent_nam)\n",
    "    return all_sentences_tok,all_indexes,all_filenames\n",
    "    \n",
    "all_unmatched_fce_tok_sent,all_unmatched_fce_indexes, all_unmatched_fce_filenames = extract_tokens_idx_and_filenames(unmatched_sentences_fce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a258dd0-0c55-40f9-912c-badd0ce8d24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_unmatched_dev_tok_sent = []\n",
    "filtered_unmatched_fce_indexes = []\n",
    "filtered_unmatched_fce_filenames = []\n",
    "\n",
    "for sentence, idx, name in zip(all_unmatched_fce_tok_sent,all_unmatched_fce_indexes, all_unmatched_fce_filenames):\n",
    "    if len(sentence) > 0 and len(idx)>0 and len(name) > 0:\n",
    "        filtered_unmatched_dev_tok_sent.append(sentence)\n",
    "        filtered_unmatched_fce_indexes.append(idx)\n",
    "        filtered_unmatched_fce_filenames.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c46f2845-d307-4930-8546-99da280a9de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_as_single_strings_fce = []\n",
    "prev_cleaned = None\n",
    "prev_clean_indices = None\n",
    "prev_filename = None\n",
    "\n",
    "for sentence, idx_list, filename in zip(filtered_unmatched_dev_tok_sent, filtered_unmatched_fce_indexes, filtered_unmatched_fce_filenames):\n",
    "    cleaned = ''\n",
    "    clean_indices = ''\n",
    "    \n",
    "    for idx, token in zip(idx_list, sentence):\n",
    "        clean_token = token.replace('\\\\\\\\\\\\', '')\n",
    "        cleaned += clean_token\n",
    "        clean_indices += idx * len(clean_token)\n",
    "\n",
    "    sentences_as_single_strings_fce.append((filename[0], cleaned, clean_indices))\n",
    "    \n",
    "    if prev_cleaned is not None and prev_clean_indices is not None:\n",
    "        combined_cleaned = prev_cleaned + cleaned\n",
    "        combined_indices = prev_clean_indices + clean_indices\n",
    "        sentences_as_single_strings_fce.append((prev_filename[0], combined_cleaned, combined_indices))\n",
    "    \n",
    "    prev_cleaned = cleaned\n",
    "    prev_clean_indices = clean_indices\n",
    "    prev_filename = filename\n",
    "\n",
    "    sentences_as_single_strings_fce.append((filename[0], cleaned, clean_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88eb44da-4e13-46f7-9b0e-38e896ef7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_as_single_strings_multiged_dev = []\n",
    "\n",
    "for sentence in unmatched_multiged_dev:\n",
    "    cleaned_sentence = ''\n",
    "    sentence_indices = ''  \n",
    "    clean_indices = ''\n",
    "    for token in sentence:\n",
    "        token_text = token[1]  \n",
    "        token_idx = token[0] \n",
    "        token_label = token[-1]\n",
    "        if token_text == '\\\\\"':\n",
    "            cleaned_sentence += '\"'\n",
    "        else:\n",
    "            cleaned_sentence += token_text\n",
    "\n",
    "        sentence_indices += token_idx * len(token_text)\n",
    "        cleaned_labels = ''.join(token[-1].replace('\\\\\\\\\\\\', '') for token in sentence)\n",
    "    sentences_as_single_strings_multiged_dev.append((sentence_indices,cleaned_sentence, cleaned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17d7f19f-5cb1-41a8-a58a-e3f8d3a27ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_token_ids(index_string):\n",
    "    return re.findall(r'T\\d+', index_string)\n",
    "\n",
    "def debug_token_ids(sentence, index_string, label=\"\"):\n",
    "    token_ids = split_token_ids(index_string)\n",
    "    print(f\"\\n--- DEBUG TOKEN IDS ({label}) ---\")\n",
    "    print(f\"Sentence length: {len(sentence)}\")\n",
    "    print(f\"Index string length: {len(index_string)}\")\n",
    "    print(f\"Number of token IDs: {len(token_ids)}\")\n",
    "    print(\"Sentence (repr):\", repr(sentence))\n",
    "    print(\"Token IDs (first 50):\", token_ids[:50])\n",
    "    print(\"--- END DEBUG ---\\n\")\n",
    "\n",
    "def get_charwise_token_ids(sentence1, sentence2, indexes1):\n",
    "    token_ids = split_token_ids(indexes1)\n",
    "\n",
    "    if len(token_ids) != len(sentence1):\n",
    "        if sentence1 and sentence1[0] in {' ', '\\n'} and len(token_ids) == len(sentence1) - 1:\n",
    "            sentence1 = sentence1[1:]\n",
    "        elif sentence1 and sentence1[-1] in {' ', '\\n'} and len(token_ids) == len(sentence1) - 1:\n",
    "            sentence1 = sentence1[:-1]\n",
    "\n",
    "        if len(token_ids) != len(sentence1):\n",
    "            debug_token_ids(sentence1, indexes1, label=\"FCE Sentence Token Alignment\")\n",
    "            raise ValueError(\n",
    "                f\"Mismatch between characters and token IDs: {len(sentence1)} chars vs {len(token_ids)} IDs\"\n",
    "            )\n",
    "\n",
    "    start = sentence1.find(sentence2)\n",
    "    if start == -1:\n",
    "        raise ValueError(\n",
    "            f\"Sentence2 not found in sentence1:\\n→ sentence1: {sentence1}\\n→ sentence2: {sentence2}\"\n",
    "        )\n",
    "\n",
    "    end = start + len(sentence2)\n",
    "    return ''.join(token_ids[start:end])\n",
    "\n",
    "def find_and_remove_overlapping_matches(not_matching_any, dev_remaining_unmatched, threshold=0.8, normalize=False):\n",
    "    results = set()\n",
    "    matched_not_any = set()\n",
    "    matched_dev = set()\n",
    "\n",
    "    for file_name, sentence1, indexes1 in not_matching_any:\n",
    "        found_match = False\n",
    "\n",
    "        for multi_idx, sentence2, gold_label in dev_remaining_unmatched:\n",
    "            if sentence2 in matched_dev:\n",
    "                continue\n",
    "\n",
    "            if sentence2.strip() == \"!\":\n",
    "                continue\n",
    "\n",
    "            s1 = sentence1.strip().replace('\\n', '') if normalize else sentence1\n",
    "            s2 = sentence2.strip().replace('\\n', '') if normalize else sentence2\n",
    "\n",
    "            similarity = ratio(s1, s2) / 100.0\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                if s2 in s1:\n",
    "                    print(f\"\\n Match in '{file_name}' — similarity: {round(similarity, 3)}\")\n",
    "                    print(f\"→ Sentence 1 (FCE): {s1}\")\n",
    "                    print(f\"→ Sentence 2 (MultiGED):  {s2}\")\n",
    "\n",
    "                    try:\n",
    "                        overlap_index = get_charwise_token_ids(s1, s2, indexes1)\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Skipping due to index mismatch: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    results.add((file_name, overlap_index, sentence2, gold_label))\n",
    "                    matched_not_any.add((file_name, sentence1, indexes1))\n",
    "                    matched_dev.add(sentence2)\n",
    "                    found_match = True\n",
    "                    break  \n",
    "\n",
    "        if found_match:\n",
    "            continue\n",
    "\n",
    "    remaining_not_matching_any = [\n",
    "        item for item in not_matching_any if item not in matched_not_any\n",
    "    ]\n",
    "    remaining_dev = [\n",
    "        item for item in dev_remaining_unmatched if item[1] not in matched_dev\n",
    "    ]\n",
    "\n",
    "    return results, remaining_not_matching_any, remaining_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "400090b6-38fc-4ccc-8984-8fdc8f646a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Match in 'doc562.xml' — similarity: 0.85\n",
      "→ Sentence 1 (FCE): Areyouafraid?Well,youcanlookatyourhouse...Nowadays,wehaveallkindofinstrumentstowashingupthedishes,tocooktobemorecomfortable,forexampletheTV,thevideorecorder,etc.\n",
      "→ Sentence 2 (MultiGED):  Nowadays,wehaveallkindofinstrumentstowashingupthedishes,tocooktobemorecomfortable,forexampletheTV,thevideorecorder,etc.\n",
      "\n",
      " Match in 'doc562.xml' — similarity: 0.822\n",
      "→ Sentence 1 (FCE): Maybe,inafewyearslaterwe'llhaveabiggerTV,amoresuitableradiobut,frommypointofview,Ithinkthesefactswillnotchangethewayofourlifes.Wewillbeabletoacceleratetheprocessofthecomputer'sgamesforexample...Butnotinthestrangewayyoucanimagine...ImeanIdon'tthinkwe'llchangecarsbyaereoplanes...\n",
      "→ Sentence 2 (MultiGED):  Maybe,inafewyearslaterwe'llhaveabiggerTV,amoresuitableradiobut,frommypointofview,Ithinkthesefactswillnotchangethewayofourlifes.Wewillbeabletoacceleratetheprocessofthecomputer'sgamesforexample...\n",
      "\n",
      " Match in 'doc562.xml' — similarity: 0.475\n",
      "→ Sentence 1 (FCE): Wewillbeabletoacceleratetheprocessofthecomputer'sgamesforexample...Butnotinthestrangewayyoucanimagine...ImeanIdon'tthinkwe'llchangecarsbyaereoplanes...\n",
      "→ Sentence 2 (MultiGED):  ImeanIdon'tthinkwe'llchangecarsbyaereoplanes...\n",
      "\n",
      " Match in 'doc190.xml' — similarity: 1.0\n",
      "→ Sentence 1 (FCE): TheshowisonTuesdaythe14thbetween10.00-13.00.\n",
      "→ Sentence 2 (MultiGED):  TheshowisonTuesdaythe14thbetween10.00-13.00.\n",
      "\n",
      " Match in 'doc190.xml' — similarity: 0.635\n",
      "→ Sentence 1 (FCE): Wethinkisthegreatopportunitytoseelatestfashion,findaboutmakeupandhairstyles.Andallofitisfree,AccordingtoyourprogrammewehavethescienceMuseumwhichisinthemorningandthenshopping.\n",
      "→ Sentence 2 (MultiGED):  AccordingtoyourprogrammewehavethescienceMuseumwhichisinthemorningandthenshopping.\n",
      "\n",
      " Match in 'doc2302.xml' — similarity: 0.527\n",
      "→ Sentence 1 (FCE): Itiscalled,,TheLondonFashionandLeisureShow.\"Thiswouldbeagreatopportunitybecauseweareallintrestedinclothes,sportsandfashion.\n",
      "→ Sentence 2 (MultiGED):  Itiscalled,,TheLondonFashionandLeisureShow.\"\n",
      "\n",
      " Match in 'doc1797.xml' — similarity: 0.797\n",
      "→ Sentence 1 (FCE): Iwasthere,Isawit.Ihopedthatit,justcouldsee3people,butitwasmistake...Patsawit!\n",
      "→ Sentence 2 (MultiGED):  Ihopedthatit,justcouldsee3people,butitwasmistake...\n",
      "\n",
      " Match in 'doc1797.xml' — similarity: 0.411\n",
      "→ Sentence 1 (FCE): Hepromisedtokeepsecret,inthenextmorningwehadarguimentwithPat,3ofuswereshoutingateachother,inshorterPatwentawayfromus.Twodayslaterinthemorning,thepolicemancametoPeter...So,nowmybestfriendisprisoner.\n",
      "→ Sentence 2 (MultiGED):  Twodayslaterinthemorning,thepolicemancametoPeter...\n",
      "\n",
      " Match in 'doc1797.xml' — similarity: 0.532\n",
      "→ Sentence 1 (FCE): Twodayslaterinthemorning,thepolicemancametoPeter...So,nowmybestfriendisprisoner.\n",
      "→ Sentence 2 (MultiGED):  So,nowmybestfriendisprisoner.\n",
      "\n",
      " Match in 'doc1874.xml' — similarity: 0.736\n",
      "→ Sentence 1 (FCE): Idon'tthinkthereisanythingI'dliketochange.I'mverypleasedwithmysituationandIhopeyouarepleasedwithyours!,\n",
      "→ Sentence 2 (MultiGED):  I'mverypleasedwithmysituationandIhopeyouarepleasedwithyours!\n",
      "\n",
      " Match in 'doc720.xml' — similarity: 0.923\n",
      "→ Sentence 1 (FCE): Yeah!!!\n",
      "→ Sentence 2 (MultiGED):  Yeah!!\n",
      "\n",
      " Match in 'doc2535.xml' — similarity: 0.725\n",
      "→ Sentence 1 (FCE): Icouldn'tbelievewhenhesaid:\"O.K.ifyouwanttohelp,hereisalistofwhatyoucando.\"Myjobwastoleavesomeleafletsonshops,restaurantsandstreets.\n",
      "→ Sentence 2 (MultiGED):  Icouldn'tbelievewhenhesaid:\"O.K.ifyouwanttohelp,hereisalistofwhatyoucando.\"\n",
      "\n",
      " Match in 'doc857.xml' — similarity: 0.926\n",
      "→ Sentence 1 (FCE): Yourssincerely,Ihaveaskedtowriteacompositionregardingthequestionitisbelievedthatshoppingisnotalwaysenjoyable.\n",
      "→ Sentence 2 (MultiGED):  Ihaveaskedtowriteacompositionregardingthequestionitisbelievedthatshoppingisnotalwaysenjoyable.\n"
     ]
    }
   ],
   "source": [
    "results, remaining_fce, remaining_multiged = find_and_remove_overlapping_matches(\n",
    "    sentences_as_single_strings_fce,\n",
    "    sentences_as_single_strings_multiged_dev,\n",
    "    threshold=0.40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22b4c7ea-c18f-4c88-9852-44b9ec2623c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\n",
    "list_res= list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5755312-3938-49ac-a885-37e15de0a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_list):\n",
    "    output = []\n",
    "\n",
    "    for data in data_list:\n",
    "        filename, indices, sentence, gold_labels = data\n",
    "\n",
    "        numeric_indices = []\n",
    "        i = 0\n",
    "        while i < len(indices):\n",
    "            if indices[i] == 'T':\n",
    "                i += 1\n",
    "                num_str = ''\n",
    "                while i < len(indices) and indices[i].isdigit():\n",
    "                    num_str += indices[i]\n",
    "                    i += 1\n",
    "                if num_str:\n",
    "                    numeric_indices.append(num_str)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        if not numeric_indices:\n",
    "            continue\n",
    "\n",
    "        grouped_indices = []\n",
    "        current_group = [numeric_indices[0]]\n",
    "        for num in numeric_indices[1:]:\n",
    "            if num == current_group[-1]:\n",
    "                current_group.append(num)\n",
    "            else:\n",
    "                grouped_indices.append(current_group)\n",
    "                current_group = [num]\n",
    "        grouped_indices.append(current_group)\n",
    "\n",
    "        if len(grouped_indices) != len(gold_labels):\n",
    "            print(f\"Skipping {filename} - Group/Label mismatch \"\n",
    "                  f\"({len(grouped_indices)} vs {len(gold_labels)})\")\n",
    "            continue\n",
    "\n",
    "        total_span = sum(len(group) for group in grouped_indices)\n",
    "        if total_span != len(sentence):\n",
    "            print(f\"Skipping {filename} - Span mismatch: sentence len {len(sentence)}, total span {total_span}\")\n",
    "            continue\n",
    "\n",
    "        start = 0\n",
    "        word_index_pairs = []\n",
    "\n",
    "        for idx, group in enumerate(grouped_indices):\n",
    "            word_length = len(group)\n",
    "            word = sentence[start:start + word_length]\n",
    "            unique_index = f\"T{group[0]}\"\n",
    "            label = gold_labels[idx]\n",
    "            word_index_pairs.append([unique_index, word, label])\n",
    "            start += word_length\n",
    "\n",
    "        output.append({filename: word_index_pairs})\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e48abdd-fed3-41b0-b003-772612b6f0bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping doc857.xml - Group/Label mismatch (20 vs 19)\n",
      "Skipping doc190.xml - Group/Label mismatch (12 vs 11)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "processed_matches2 =process_data(results)\n",
    "print(len(processed_matches2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6511ab95-fbdf-4978-a892-63e9f41e4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = []\n",
    "\n",
    "for pr_sentence in processed_matches2:\n",
    "    for pr_filename, pr_lines in pr_sentence.items():\n",
    "        pr_keys = {(tid, tok) for tid, tok, _ in pr_lines}\n",
    "        pr_labels = {(tid, tok): label for tid, tok, label in pr_lines}\n",
    "\n",
    "        matched = False \n",
    "\n",
    "        for dev_sentence in unmatched_sentences_fce:\n",
    "            if not dev_sentence:\n",
    "                continue\n",
    "            if dev_sentence[0][0] != pr_filename:\n",
    "                continue\n",
    "\n",
    "            dev_keys = {(tok[1], tok[2]) for tok in dev_sentence}\n",
    "\n",
    "            if not pr_keys.issubset(dev_keys):\n",
    "                continue\n",
    "\n",
    "            updated_sentence = []\n",
    "            for tok in dev_sentence:\n",
    "                tid, token = tok[1], tok[2]\n",
    "                label = pr_labels.get((tid, token), '')  \n",
    "                updated_sentence.append(tok + [label])\n",
    "\n",
    "            all_.append(updated_sentence)\n",
    "            matched = True\n",
    "            break  \n",
    "\n",
    "        if not matched:\n",
    "            print(f\"No full match found for: {pr_filename} with tokens {pr_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "07176d2a-b932-41b8-958a-0e18c68e5bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_data_final = []\n",
    "filtered_data_final2 = []\n",
    "\n",
    "\n",
    "for sentence in all_:\n",
    "    seen_rows = set()\n",
    "    filtered_sentence = []\n",
    "    for row in sentence:\n",
    "        if row[-1] != '':\n",
    "            row_tuple = tuple(row)\n",
    "            if row_tuple not in seen_rows:\n",
    "                filtered_sentence.append(row)\n",
    "                seen_rows.add(row_tuple)\n",
    "    filtered_data_final.append(filtered_sentence)\n",
    "\n",
    "\n",
    "for sentence in filtered_data_final:\n",
    "    if len(sentence) >= 2 and sentence[0][2] == 'the' and sentence[1][2] == 'Two':\n",
    "        filtered_data_final2.append(sentence[1:])\n",
    "    else:\n",
    "        filtered_data_final2.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "161762e7-3c57-4228-acbb-fe840fdf8fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['doc2302.xml', 'T0', 'It', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T1', 'is', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T2', 'called', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T3', ',', 'i', '\"', 'RP', 'c'],\n",
       "  ['doc2302.xml', 'T4', ',', 'i', '\"', 'RP', 'c'],\n",
       "  ['doc2302.xml', 'T5', 'The', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T6', 'London', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T7', 'Fashion', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T8', 'and', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T9', 'Leisure', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T10', 'Show', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T11', '.', 'c', '', '', 'c'],\n",
       "  ['doc2302.xml', 'T0', '\"', 'c', '', '', 'c']],\n",
       " [['doc1797.xml', 'T0', 'Two', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T1', 'days', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T2', 'later', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T3', 'in', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T4', 'the', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T5', 'morning', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T6', ',', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T7', 'the', 'i', 'a', 'RD', 'i'],\n",
       "  ['doc1797.xml', 'T8', 'policeman', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T9', 'came', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T10', 'to', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T11', 'Peter', 'c', '', '', 'i'],\n",
       "  ['doc1797.xml', 'T12', '...', 'c', '', '', 'c']],\n",
       " [['doc562.xml', 'T0', 'Maybe', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T1', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T2', 'in', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T3', 'a', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T4', 'few', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T5', 'years', 'i', \"years'\", 'MP', 'c'],\n",
       "  ['doc562.xml', 'T6', 'later', 'i', 'time', 'R', 'i'],\n",
       "  ['doc562.xml', 'T7', 'we', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T8', \"'ll\", 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T9', 'have', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T10', 'a', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T11', 'bigger', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T12', 'TV', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T13', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T14', 'a', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T15', 'more', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T16', 'suitable', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T17', 'radio', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T18', 'but', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T19', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T20', 'from', 'i', 'in my opinion', 'ID', 'i'],\n",
       "  ['doc562.xml', 'T21', 'my', 'i', 'in my opinion', 'ID', 'i'],\n",
       "  ['doc562.xml', 'T22', 'point', 'i', 'in my opinion', 'ID', 'i'],\n",
       "  ['doc562.xml', 'T23', 'of', 'i', 'in my opinion', 'ID', 'i'],\n",
       "  ['doc562.xml', 'T24', 'view', 'i', 'in my opinion', 'ID', 'i'],\n",
       "  ['doc562.xml', 'T25', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T26', 'I', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T27', 'think', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T28', 'these', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T29', 'facts', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T30', 'will', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T31', 'not', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T32', 'change', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T33', 'the', 'i', 'our', 'RD', 'i'],\n",
       "  ['doc562.xml', 'T34', 'way', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T35', 'of', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T36', 'our', 'i', '', 'UD', 'c'],\n",
       "  ['doc562.xml', 'T37', 'lifes', 'i', 'lives', 'IN', 'i'],\n",
       "  ['doc562.xml', 'T38', '.', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T0', 'We', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T1', 'will', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T2', 'be', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T3', 'able', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T4', 'to', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T5', 'accelerate', 'i', 'speed up', 'RV', 'i'],\n",
       "  ['doc562.xml', 'T6', 'the', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T7', 'process', 'i', 'running', 'RN', 'i'],\n",
       "  ['doc562.xml', 'T8', 'of', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T9', 'the', 'i', '', 'UD', 'c'],\n",
       "  ['doc562.xml', 'T10', 'computer', 'i', 'computer', 'UP', 'c'],\n",
       "  ['doc562.xml', 'T11', \"'s\", 'i', 'computer', 'UP', 'c'],\n",
       "  ['doc562.xml', 'T12', 'games', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T13', 'for', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T14', 'example', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T15', '...', 'c', '', '', 'c']],\n",
       " [['doc562.xml', 'T9', 'Nowadays', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T10', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T11', 'we', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T12', 'have', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T13', 'all', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T14', 'kind', 'i', 'kinds', 'AGN', 'i'],\n",
       "  ['doc562.xml', 'T15', 'of', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T16', 'instruments', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T17', 'to', 'i', 'for', 'RT', 'i'],\n",
       "  ['doc562.xml', 'T18', 'washing', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T19', 'up', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T20', 'the', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T21', 'dishes', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T22', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T23', 'to', 'i', 'cooking', 'FV', 'i'],\n",
       "  ['doc562.xml', 'T24', 'cook', 'i', 'cooking', 'FV', 'i'],\n",
       "  ['doc562.xml', 'T25', 'to', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T26', 'be', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T27', 'more', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T28', 'comfortable', 'i', 'convenient', 'RJ', 'i'],\n",
       "  ['doc562.xml', 'T29', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T30', 'for', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T31', 'example', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T32', 'the', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T33', 'TV', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T34', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T35', 'the', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T36', 'video', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T37', 'recorder', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T38', ',', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T39', 'etc', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T40', '.', 'c', '', '', 'c']],\n",
       " [['doc1874.xml', 'T0', 'I', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T1', \"'m\", 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T2', 'very', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T3', 'pleased', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T4', 'with', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T5', 'my', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T6', 'situation', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T7', 'and', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T8', 'I', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T9', 'hope', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T10', 'you', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T11', 'are', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T12', 'pleased', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T13', 'with', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T14', 'yours', 'c', '', '', 'c'],\n",
       "  ['doc1874.xml', 'T15', '!', 'c', '', '', 'c']],\n",
       " [['doc720.xml', 'T0', 'Yeah', 'c', '', '', 'c'],\n",
       "  ['doc720.xml', 'T1', '!', 'c', '', '', 'c'],\n",
       "  ['doc720.xml', 'T0', '!', 'c', '', '', 'c']],\n",
       " [['doc190.xml', 'T7', 'According', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T8', 'to', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T9', 'your', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T10', 'programme', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T11', 'we', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T12', 'have', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T13', 'the', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T14', 'science', 'i', 'Science', 'RP', 'i'],\n",
       "  ['doc190.xml', 'T15', 'Museum', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T16', 'which', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T17', 'is', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T18', 'in', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T19', 'the', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T20', 'morning', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T21', 'and', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T22', 'then', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T23', 'shopping', 'c', '', '', 'c'],\n",
       "  ['doc190.xml', 'T24', '.', 'c', '', '', 'c']],\n",
       " [['doc562.xml', 'T26', 'I', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T27', 'mean', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T28', 'I', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T29', 'do', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T30', \"n't\", 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T31', 'think', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T32', 'we', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T33', \"'ll\", 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T34', 'change', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T35', 'cars', 'c', '', '', 'c'],\n",
       "  ['doc562.xml', 'T36', 'by', 'i', 'into', 'RT', 'i'],\n",
       "  ['doc562.xml', 'T37', 'aereoplanes', 'i', 'aeroplanes', 'S', 'i'],\n",
       "  ['doc562.xml', 'T38', '...', 'c', '', '', 'c']],\n",
       " [['doc1797.xml', 'T0', 'I', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T1', 'hoped', 'i', 'thought', 'RV', 'i'],\n",
       "  ['doc1797.xml', 'T2', 'that', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T3', 'it', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T4', ',', 'i', '', 'UP', 'c'],\n",
       "  ['doc1797.xml', 'T5', 'just', 'i', 'only', 'RY', 'i'],\n",
       "  ['doc1797.xml', 'T6', 'could', 'c', '', '', 'i'],\n",
       "  ['doc1797.xml', 'T7', 'see', 'i', 'be seen', 'TV', 'i'],\n",
       "  ['doc1797.xml', 'T8', '3', 'c', '', '', 'i'],\n",
       "  ['doc1797.xml', 'T9', 'people', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T10', ',', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T11', 'but', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T12', 'it', 'i', 'that', 'RA', 'i'],\n",
       "  ['doc1797.xml', 'T13', 'was', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T14', 'mistake', 'c', '', '', 'i'],\n",
       "  ['doc1797.xml', 'T15', '...', 'c', '', '', 'c']],\n",
       " [['doc1797.xml', 'T13', 'So', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T14', ',', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T15', 'now', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T16', 'my', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T17', 'best', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T18', 'friend', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T19', 'is', 'c', '', '', 'c'],\n",
       "  ['doc1797.xml', 'T20', 'prisoner', 'c', '', '', 'i'],\n",
       "  ['doc1797.xml', 'T21', '.', 'c', '', '', 'c']],\n",
       " [['doc2535.xml', 'T0', 'I', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T1', 'could', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T2', \"n't\", 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T3', 'believe', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T4', 'when', 'c', '', '', 'i'],\n",
       "  ['doc2535.xml', 'T5', 'he', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T6', 'said', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T7', ':', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T8', '\"', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T9', 'O.K.', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T10', 'if', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T11', 'you', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T12', 'want', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T13', 'to', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T14', 'help', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T15', ',', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T16', 'here', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T17', 'is', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T18', 'a', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T19', 'list', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T20', 'of', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T21', 'what', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T22', 'you', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T23', 'can', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T24', 'do', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T25', '.', 'c', '', '', 'c'],\n",
       "  ['doc2535.xml', 'T0', '\"', 'c', '', '', 'c']]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8a651db4-cf0f-4a6e-bf68-81f7ee45335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_matches_spacy_idk.extend(filtered_data_final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "995a381e-a96e-497c-8fb8-e0b429a5f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_matches_spacy_idk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b34a87e-1321-448b-aa46-8a0c118558a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev sentences still to match: 31\n",
      "Total unmatched FCE sentences: 68793\n"
     ]
    }
   ],
   "source": [
    "all_matched_sequences = set(\n",
    "    tuple(token[2] for token in sentence)\n",
    "    for sentence in dev_matches_spacy_idk\n",
    ")\n",
    "final_unmatched_multiged_dev = []\n",
    "for sentence in multiged_dev_info:\n",
    "    token_sequence = tuple(token[1] for token in sentence)\n",
    "    if token_sequence not in all_matched_sequences:\n",
    "        final_unmatched_multiged_dev.append(sentence)\n",
    "\n",
    "print(f\"Dev sentences still to match: {len(final_unmatched_multiged_dev)}\")\n",
    "\n",
    "new_all_matched=dev_matches_spacy_idk\n",
    "new_unmatched_sentences_fce = []\n",
    "for fce_sentence in combined_sentences_2or3:\n",
    "    token_sequence = tuple(token_info[3] for token_info in fce_sentence)\n",
    "    if token_sequence not in new_all_matched:\n",
    "        new_unmatched_sentences_fce.append(fce_sentence)\n",
    "\n",
    "print(f\"Total unmatched FCE sentences: {len(new_unmatched_sentences_fce)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56b8f02c-bf59-4ce9-86c8-2847ab92695c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('doc2231.xml', 'T0', 'Although', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'in', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'those', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'days', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'the', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'only', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'way', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', 'of', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T8', 'learning', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T9', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T10', 'reading', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T11', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'These', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'days', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'we', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'can', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'use', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'computer', 'c', '', '', 'i'),\n",
       "  ('doc2231.xml', 'T6', ',', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', 'television', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T8', 'and', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T9', 'some', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T10', 'sophisticated', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T11', 'equipment', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T12', '.', 'i', ',', 'RP', 'c'),\n",
       "  ('doc2231.xml', 'T0', 'which', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'were', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'unusuall', 'i', 'unusual', 'DJ', 'i'),\n",
       "  ('doc2231.xml', 'T3', 'before', 'i', 'once', 'RY', 'i'),\n",
       "  ('doc2231.xml', 'T4', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'Than', 'i', 'Then', 'SX', 'i'),\n",
       "  ('doc2231.xml', 'T1', 'to', 'i', 'with', 'RT', 'i'),\n",
       "  ('doc2231.xml', 'T2', 'technology', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'we', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'can', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'reach', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'any', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', 'place', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T8', 'in', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T9', 'the', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T10', 'world', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T11', 'easily', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T12', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'We', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'can', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'get', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'some', 'i', '', 'UQ', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'information', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'more', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'quickly', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'Technology', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'has', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'a', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'huge', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'effect', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'on', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'our', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', 'spare', 'i', 'spare-time', 'MP', 'c'),\n",
       "  ('doc2231.xml', 'T8', 'time', 'i', 'spare-time', 'MP', 'c'),\n",
       "  ('doc2231.xml', 'T9', 'activities', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T10', 'as', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T11', 'well', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T12', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'Children', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'play', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'wit', 'i', 'with', 'SX', 'i'),\n",
       "  ('doc2231.xml', 'T3', 'computer', 'i', 'computers', 'FN', 'i'),\n",
       "  ('doc2231.xml', 'T4', 'instead', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'of', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'usuall', 'i', 'usual', 'S', 'i'),\n",
       "  ('doc2231.xml', 'T7', 'plays', 'i', 'toys', 'RN', 'i'),\n",
       "  ('doc2231.xml', 'T8', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'Despite', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'the', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'huge', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'facilities', 'i', 'advantages', 'RN', 'i'),\n",
       "  ('doc2231.xml', 'T4', 'technology', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'has', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'affected', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', 'our', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T8', 'lives', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T9', 'negatively', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T10', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'There', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'has', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'been', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'change', 'c', '', '', 'i'),\n",
       "  ('doc2231.xml', 'T4', 'relatinships', 'i', 'relationships', 'S', 'i'),\n",
       "  ('doc2231.xml', 'T5', 'between', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'people', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'People', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'are', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'getting', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'more', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'and', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'more', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'lonely', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'We', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'have', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'noticed', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'the', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'environmential', 'i', 'environmental', 'S', 'i'),\n",
       "  ('doc2231.xml', 'T5', 'damage', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'in', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', 'recent', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T8', 'years', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T9', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'While', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'we', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'are', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'using', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'technology', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', ',', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'we', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', 'should', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T8', 'take', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T9', 'into', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T10', 'consideration', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T11', 'environment', 'c', '', '', 'i'),\n",
       "  ('doc2231.xml', 'T12', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'We', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'should', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', \"n't\", 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'allow', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'to', 'i', 'the world to be exploited', 'W', 'i'),\n",
       "  ('doc2231.xml', 'T5', 'be', 'i', 'the world to be exploited', 'W', 'i'),\n",
       "  ('doc2231.xml',\n",
       "   'T6',\n",
       "   'exploited',\n",
       "   'i',\n",
       "   'the world to be exploited',\n",
       "   'W',\n",
       "   'i'),\n",
       "  ('doc2231.xml', 'T7', 'the', 'i', 'the world to be exploited', 'W', 'i'),\n",
       "  ('doc2231.xml', 'T8', 'world', 'i', 'the world to be exploited', 'W', 'i'),\n",
       "  ('doc2231.xml', 'T9', '.', 'c', '', '', 'c')],\n",
       " [('doc2231.xml', 'T0', 'If', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T1', 'we', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T2', 'want', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T3', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T4', 'leave', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T5', 'a', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T6', 'good', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T7', 'world', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T8', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T9', 'the', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T10', 'next', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T11', 'generations', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T12', ',', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T13', 'we', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T14', 'should', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T15', 'behave', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T16', 'more', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T17', 'respectfully', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T18', 'againgst', 'i', 'against', 'S', 'i'),\n",
       "  ('doc2231.xml', 'T19', 'the', 'i', '', 'UD', 'c'),\n",
       "  ('doc2231.xml', 'T20', 'nature', 'c', '', '', 'c'),\n",
       "  ('doc2231.xml', 'T21', '.', 'c', '', '', 'c')],\n",
       " [('doc869.xml', 'T0', 'Dear', 'c', '', '', 'c'),\n",
       "  ('doc869.xml', 'T1', 'Sir', 'c', '', '', 'c'),\n",
       "  ('doc869.xml', 'T2', '/', 'c', '', '', 'c'),\n",
       "  ('doc869.xml', 'T3', 'Madam', 'c', '', '', 'c'),\n",
       "  ('doc869.xml', 'T4', ':', 'c', '', '', 'c')],\n",
       " [('doc684.xml', 'T0', 'Dear', 'c', '', '', 'c'),\n",
       "  ('doc684.xml', 'T1', 'Sir', 'c', '', '', 'c'),\n",
       "  ('doc684.xml', 'T2', '/', 'c', '', '', 'c'),\n",
       "  ('doc684.xml', 'T3', 'Madam', 'c', '', '', 'c'),\n",
       "  ('doc684.xml', 'T4', ':', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'am', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'writing', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'complain', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'about', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'musical', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'show', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', '.', 'i', ',', 'RP', 'c')],\n",
       " [('doc1962.xml', 'T0', 'Over', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'Rainbow', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'which', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'you', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'advertised', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'saw', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'show', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'at', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'your', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'theatre', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'with', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'my', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'friend', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'on', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', '6th', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'June', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'However', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'was', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'disappointed', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'with', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'something', 'i', '', 'R', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'which', 'i', '', 'R', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'did', 'i', '', 'R', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'not', 'i', '', 'R', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'match', 'i', '', 'R', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'your', 'i', '', 'R', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'advertisement', 'i', '', 'R', 'c'),\n",
       "  ('doc1962.xml', 'T13', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'First', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'of', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'all', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'would', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'like', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'have', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'seen', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'Danny', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'Brook', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'as', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T14', 'am', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T15', 'keen', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T16', 'on', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T17', 'his', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T18', 'play', 'i', 'acting', 'RN', 'i'),\n",
       "  ('doc1962.xml', 'T19', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'He', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'did', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'not', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'play', 'i', 'perform', 'RV', 'i'),\n",
       "  ('doc1962.xml', 'T4', 'in', 'i', '', 'UT', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'the', 'i', 'that', 'RD', 'i'),\n",
       "  ('doc1962.xml', 'T6', 'evening', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'It', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'made', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'me', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'sad', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'Secondly', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'starting', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'time', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'has', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'been', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'changed', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'from', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', '19:30', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', '20:15', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'We', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'waited', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'for', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'ages', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'because', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'we', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'had', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'not', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'been', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'informed', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'In', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'addition', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'although', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'you', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'advertised', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'the', 'i', 'that', 'R', 'i'),\n",
       "  ('doc1962.xml', 'T7', 'discounts', 'i', 'discount', 'IJ', 'i'),\n",
       "  ('doc1962.xml', 'T8', 'tickets', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'available', 'c', '', '', 'i'),\n",
       "  ('doc1962.xml', 'T10', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'they', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'do', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', 'not', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T14', 'sale', 'i', 'sell', 'DV', 'i'),\n",
       "  ('doc1962.xml', 'T15', 'them', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T16', 'any', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T17', 'more', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T18', 'according', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T19', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T20', 'your', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T21', 'staff', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T22', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'Finally', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'we', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'went', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'restaurant', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'in', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'your', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'theater', 'i', 'theatre', 'SA', 'i'),\n",
       "  ('doc1962.xml', 'T10', 'after', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'show', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'It', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'was', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'closed', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'because', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'of', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'staff', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'trainning', 'i', 'training', 'S', 'i'),\n",
       "  ('doc1962.xml', 'T8', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'am', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'sure', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'you', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'can', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'understand', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'my', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'disappointment', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', '.', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'must', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'insist', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'that', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'you', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'refund', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'cost', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'of', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'show', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'Unless', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'hear', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'from', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'you', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'within', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'ten', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'days', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'shall', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'have', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', 'take', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T14', 'legal', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T15', 'advice', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T16', 'on', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T17', 'the', 'i', 'this', 'RD', 'i'),\n",
       "  ('doc1962.xml', 'T18', 'matter', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T19', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'look', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'forward', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'hearing', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'from', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'you', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'in', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'the', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'near', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'future', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'It', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'is', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'believed', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'that', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'our', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'life', 'i', 'lifestyle', 'RP', 'i'),\n",
       "  ('doc1962.xml', 'T6', 'style', 'i', 'lifestyle', 'RP', 'i'),\n",
       "  ('doc1962.xml', 'T7', 'has', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'been', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'changed', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'by', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'mordern', 'i', 'modern', 'S', 'i'),\n",
       "  ('doc1962.xml', 'T12', 'technology', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', 'such', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T14', 'as', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T15', 'computers', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T16', 'and', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T17', 'washing', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T18', 'machine', 'i', 'machines', 'FN', 'i'),\n",
       "  ('doc1962.xml', 'T19', 'etc', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T20', '.', 'i', ', especially', 'RP', 'c')],\n",
       " [('doc1962.xml', 'T0', 'Especially', 'i', ', especially', 'RP', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'for', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'families', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'have', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'found', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'very', 'c', '', '', 'i'),\n",
       "  ('doc1962.xml', 'T4', 'expensive', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'buy', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'them', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'but', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'there', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'are', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'number', 'c', '', '', 'i'),\n",
       "  ('doc1962.xml', 'T12', 'of', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', 'advantages', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T14', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'First', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'of', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'all', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'dishwashers', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'are', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'very', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'effecient', 'i', 'efficient', 'S', 'i'),\n",
       "  ('doc1962.xml', 'T8', 'for', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'me', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'because', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'work', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', 'full', 'i', 'full-time', 'MP', 'c'),\n",
       "  ('doc1962.xml', 'T14', 'time', 'i', 'full-time', 'MP', 'c'),\n",
       "  ('doc1962.xml', 'T15', 'and', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T16', 'look', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T17', 'after', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T18', 'my', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T19', 'children', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T20', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'It', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'is', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'very', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'difficult', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'cook', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'and', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'wash', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'dishes', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'after', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'working', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'Before', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'buying', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'my', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'dishwasher', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'asked', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'my', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'son', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'wash', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'them', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'He', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'complained', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'about', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'a', 'c', '', '', 'i'),\n",
       "  ('doc1962.xml', 'T4', 'lot', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'Secondly', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'computers', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'are', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'useful', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'for', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'us', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'as', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'well', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'because', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'we', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'do', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T13', 'not', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T14', 'need', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T15', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T16', 'go', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T17', 'somewhere', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T18', 'if', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T19', 'we', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T20', 'want', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T21', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T22', 'have', 'i', 'get', 'RV', 'i'),\n",
       "  ('doc1962.xml', 'T23', 'information', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T24', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T25', 'for', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T26', 'example', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T27', 'traveling', 'i', 'travelling', 'IV', 'i'),\n",
       "  ('doc1962.xml', 'T28', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'However', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'my', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'family', 'i', 'family', 'UP', 'c'),\n",
       "  ('doc1962.xml', 'T3', \"'s\", 'i', 'family', 'UP', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'members', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'do', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml',\n",
       "   'T6',\n",
       "   'less',\n",
       "   'i',\n",
       "   'communicate with each other less',\n",
       "   'W',\n",
       "   'i'),\n",
       "  ('doc1962.xml',\n",
       "   'T7',\n",
       "   'communicate',\n",
       "   'i',\n",
       "   'communicate with each other less',\n",
       "   'W',\n",
       "   'i'),\n",
       "  ('doc1962.xml',\n",
       "   'T8',\n",
       "   'with',\n",
       "   'i',\n",
       "   'communicate with each other less',\n",
       "   'W',\n",
       "   'i'),\n",
       "  ('doc1962.xml',\n",
       "   'T9',\n",
       "   'each',\n",
       "   'i',\n",
       "   'communicate with each other less',\n",
       "   'W',\n",
       "   'i'),\n",
       "  ('doc1962.xml',\n",
       "   'T10',\n",
       "   'other',\n",
       "   'i',\n",
       "   'communicate with each other less',\n",
       "   'W',\n",
       "   'i'),\n",
       "  ('doc1962.xml', 'T11', 'than', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'we', 'i', 'they', 'RA', 'i'),\n",
       "  ('doc1962.xml', 'T13', 'used', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T14', 'to', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T15', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T16', 'because', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T17', 'my', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T18', 'children', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T19', 'are', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T20', 'keen', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T21', 'on', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T22', 'playing', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T23', 'computer', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T24', 'games', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T25', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'In', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'my', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'opinion', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', ',', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'the', 'i', '', 'UD', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'morden', 'i', 'modern', 'S', 'i'),\n",
       "  ('doc1962.xml', 'T6', 'technology', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'make', 'i', 'makes', 'AGV', 'i'),\n",
       "  ('doc1962.xml', 'T8', 'our', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'life', 'i', 'lifestyle', 'RP', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'style', 'i', 'lifestyle', 'RP', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'combinient', 'i', 'convenient', 'S', 'i'),\n",
       "  ('doc1962.xml', 'T12', '.', 'c', '', '', 'c')],\n",
       " [('doc1962.xml', 'T0', 'It', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T1', 'is', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T2', 'very', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T3', 'good', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T4', 'for', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T5', 'us', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T6', 'but', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T7', 'I', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T8', 'think', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T9', 'we', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T10', 'should', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T11', 'think', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T12', 'about', 'i', '', 'UT', 'c'),\n",
       "  ('doc1962.xml', 'T13', 'more', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T14', 'carefully', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T15', 'when', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T16', 'we', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T17', 'buy', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T18', 'something', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T19', 'for', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T20', 'our', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T21', 'children', 'c', '', '', 'c'),\n",
       "  ('doc1962.xml', 'T22', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', '17', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'June', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', '2000', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'am', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'writing', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'complain', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'about', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T6', 'a', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T7', 'musical', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T8', 'show', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T9', 'on', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T10', '10', 'i', '10th', 'MP', 'i'),\n",
       "  ('doc2572.xml', 'T11', 'of', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T12', 'June', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T13', ',', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T14', 'and', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T15', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T16', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T17', 'very', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T18', 'disappinted', 'i', 'disappointed', 'S', 'i'),\n",
       "  ('doc2572.xml', 'T19', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'promised', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'that', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'the', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'starring', 'i', 'star', 'DN', 'i'),\n",
       "  ('doc2572.xml', 'T6', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T7', 'Danny', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T8', 'Brock', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T9', 'but', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T10', 'when', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T11', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T12', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T13', 'there', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T14', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T15', 'saw', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T16', 'just', 'i', '', 'UY', 'c'),\n",
       "  ('doc2572.xml', 'T17', 'other', 'i', 'another', 'RQ', 'i'),\n",
       "  ('doc2572.xml', 'T18', 'starring', 'i', 'star', 'DN', 'i'),\n",
       "  ('doc2572.xml', 'T19', 'that', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T20', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T21', 'completely', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T22', 'difference', 'i', 'different', 'DJ', 'i'),\n",
       "  ('doc2572.xml', 'T23', 'from', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T24', 'your', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T25', 'brochar', 'i', 'brochure', 'S', 'i'),\n",
       "  ('doc2572.xml', 'T26', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'To', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'make', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'the', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'matter', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'worse', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', ',', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T6', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T7', 'was', 'i', 'had travelled', 'TV', 'i'),\n",
       "  ('doc2572.xml', 'T8', 'travelling', 'i', 'had travelled', 'TV', 'i'),\n",
       "  ('doc2572.xml', 'T9', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T10', 'London', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T11', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T12', 'see', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T13', 'this', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T14', 'famous', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T15', 'starring', 'i', 'star', 'DN', 'i'),\n",
       "  ('doc2572.xml', 'T16', 'but', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T17', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T18', 'did', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T19', \"n't\", 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T20', 'see', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T21', 'any', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T22', 'shows', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T23', 'of', 'i', 'with', 'RT', 'i'),\n",
       "  ('doc2572.xml', 'T24', 'them', 'i', 'him', 'AGA', 'i'),\n",
       "  ('doc2572.xml', 'T25', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'It', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'make', 'i', 'makes', 'AGV', 'i'),\n",
       "  ('doc2572.xml', 'T2', 'me', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'a', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'bit', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'angry', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T6', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'According', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'in', 'i', 'to', 'RT', 'i'),\n",
       "  ('doc2572.xml', 'T2', 'your', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'brochour', 'i', 'brochure', 'S', 'i'),\n",
       "  ('doc2572.xml', 'T4', ',', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'it', 'i', 'the show would start', 'AS', 'i'),\n",
       "  ('doc2572.xml', 'T6', 'was', 'i', 'the show would start', 'AS', 'i'),\n",
       "  ('doc2572.xml', 'T7', 'started', 'i', 'the show would start', 'AS', 'i'),\n",
       "  ('doc2572.xml', 'T8', 'to', 'i', 'the show would start', 'AS', 'i'),\n",
       "  ('doc2572.xml', 'T9', 'show', 'i', 'the show would start', 'AS', 'i'),\n",
       "  ('doc2572.xml', 'T10', 'at', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T11', '19.30', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T12', 'p.m.', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T13', 'but', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T14', 'it', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T15', 'was', 'i', 'started', 'TV', 'c'),\n",
       "  ('doc2572.xml', 'T16', 'started', 'i', 'started', 'TV', 'c'),\n",
       "  ('doc2572.xml', 'T17', 'at', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T18', '20.15', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T19', 'p.m.', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T20', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T21', 'wanted', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T22', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T23', 'sleep', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T24', 'and', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T25', 'very', 'c', '', '', 'i'),\n",
       "  ('doc2572.xml', 'T26', 'anoying', 'i', 'annoying', 'S', 'i'),\n",
       "  ('doc2572.xml', 'T27', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'More', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'than', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'that', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', ',', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'when', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T6', 'bought', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T7', 'the', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T8', 'ticket', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T9', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T10', 'did', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T11', \"n't\", 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T12', 'get', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T13', 'any', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T14', 'discounts', 'i', 'discount', 'FN', 'i'),\n",
       "  ('doc2572.xml', 'T15', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'had', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'pay', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'full', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'prise', 'i', 'price', 'S', 'i'),\n",
       "  ('doc2572.xml', 'T6', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'very', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'hungry', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'and', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'tired', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T6', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'You', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'said', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'that', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'it', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'a', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T6', 'perfect', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T7', 'evening', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T8', 'out', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T9', 'but', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T10', 'it', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T11', 'was', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T12', \"n't\", 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T13', 'like', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T14', 'that', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T15', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'would', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'like', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'to', 'c', '', '', 'i'),\n",
       "  ('doc2572.xml', 'T4', 'refund', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'some', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T6', 'money', 'c', '', '', 'i'),\n",
       "  ('doc2572.xml', 'T7', 'back', 'i', '', 'UY', 'c'),\n",
       "  ('doc2572.xml', 'T8', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'paid', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'for', 'i', '£ 50.00 for the show', 'W', 'i'),\n",
       "  ('doc2572.xml', 'T3', 'the', 'i', '£ 50.00 for the show', 'W', 'i'),\n",
       "  ('doc2572.xml', 'T4', 'show', 'i', '£ 50.00 for the show', 'W', 'i'),\n",
       "  ('doc2572.xml', 'T5', '£', 'i', '£ 50.00 for the show', 'W', 'i'),\n",
       "  ('doc2572.xml', 'T6', '50.00', 'i', '£ 50.00 for the show', 'W', 'i'),\n",
       "  ('doc2572.xml', 'T7', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T8', 'also', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T9', 'have', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T10', 'a', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T11', 'recite', 'i', 'receipt', 'SX', 'i'),\n",
       "  ('doc2572.xml', 'T12', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T13', 'prove', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T14', 'it', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T15', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'I', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'would', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T2', 'like', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T3', 'you', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T4', 'to', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T5', 'give', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T6', 'my', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T7', 'money', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T8', 'back', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T9', 'as', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T10', 'soon', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T11', 'as', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T12', 'possible', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T13', '.', 'c', '', '', 'c')],\n",
       " [('doc2572.xml', 'T0', 'Phukana', 'c', '', '', 'c'),\n",
       "  ('doc2572.xml', 'T1', 'Najee', 'c', '', '', 'c')]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_matches_spacy_idk[500:555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28ebcb25-fc35-4b61-89ac-0799619f3492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('T0', 'Pat', 'c'), ('T1', 'saw', 'c'), ('T2', 'it', 'c'), ('T3', '!', 'c')]\n",
      "\n",
      "[('T0', 'The', 'c'), ('T1', 'second', 'c'), ('T2', 'day', 'c'), ('T3', 'objective', 'i'), ('T4', 'was', 'c'), ('T5', 'to', 'c'), ('T6', 'install', 'c'), ('T7', 'the', 'c'), ('T8', 'scene', 'i'), ('T9', ',', 'c'), ('T10', 'with', 'c'), ('T11', 'lights', 'c'), ('T12', ',', 'c'), ('T13', 'loudspeakers', 'c'), ('T14', 'and', 'c'), ('T15', 'all', 'c'), ('T16', 'stuff', 'i'), ('T17', '.', 'c')]\n",
      "\n",
      "[('T0', 'I', 'c'), ('T1', 'noticed', 'c'), ('T2', 'that', 'c'), ('T3', 'the', 'c'), ('T4', 'artists', 'c'), ('T5', 'were', 'c'), ('T6', 'from', 'c'), ('T7', 'only', 'c'), ('T8', 'six', 'c'), ('T9', 'countries', 'c'), ('T10', 'insted', 'i'), ('T11', 'that', 'c'), ('T12', 'from', 'c'), ('T13', 'around', 'c'), ('T14', 'the', 'c'), ('T15', 'world', 'c'), ('T16', '.', 'c')]\n",
      "\n",
      "[('T0', 'I', 'c'), ('T1', 'was', 'c'), ('T2', 'promised', 'c'), ('T3', 'that', 'c'), ('T4', 'after', 'c'), ('T5', 'show', 'i'), ('T6', 'I', 'c'), ('T7', 'could', 'c'), ('T8', 'go', 'c'), ('T9', 'to', 'c'), ('T10', 'theatre', 'i'), ('T11', 'restaurant', 'c'), ('T12', 'but', 'c'), ('T13', 'due', 'c'), ('T14', 'to', 'c'), ('T15', 'started', 'i'), ('T16', 'late', 'c'), ('T17', 'we', 'i'), ('T18', 'aslo', 'i'), ('T19', 'finished', 'c'), ('T20', 'late', 'c'), ('T21', 'therefore', 'c'), ('T22', ',', 'c'), ('T23', 'when', 'c'), ('T24', 'I', 'c'), ('T25', 'went', 'c'), ('T26', 'to', 'c'), ('T27', 'the', 'c'), ('T28', 'restaurant', 'c'), ('T29', 'they', 'c'), ('T30', 'were', 'c'), ('T31', 'deffinately', 'i'), ('T32', 'closed', 'c'), ('T33', '.', 'c')]\n",
      "\n",
      "[('T0', 'Without', 'c'), ('T1', 'give', 'c'), ('T2', 'really', 'c'), ('T3', 'importance', 'c'), ('T4', 'to', 'c'), ('T5', 'the', 'c'), ('T6', 'calidad', 'i'), ('T7', 'in', 'i'), ('T8', 'reference', 'c'), ('T9', 'to', 'c'), ('T10', 'the', 'c'), ('T11', 'appearance', 'c'), ('T12', 'there', 'c'), ('T13', 'is', 'c'), ('T14', 'more', 'c'), ('T15', 'preocupation', 'c'), ('T16', 'in', 'c'), ('T17', 'the', 'c'), ('T18', 'food', 'c'), ('T19', 'area', 'c'), ('T20', ',', 'c'), ('T21', 'because', 'c'), ('T22', 'people', 'c'), ('T23', 'in', 'c'), ('T24', 'general', 'c'), ('T25', 'they', 'c'), ('T26', 'do', 'c'), ('T27', \"n't\", 'c'), ('T28', 'spent', 'i'), ('T29', 'the', 'c'), ('T30', 'time', 'c'), ('T31', 'for', 'c'), ('T32', 'prepare', 'i'), ('T33', 'food', 'c'), ('T34', 'and', 'c'), ('T35', 'when', 'c'), ('T36', 'they', 'c'), ('T37', 'shopping', 'i'), ('T38', 'they', 'c'), ('T39', 'choose', 'c'), ('T40', 'pre', 'c'), ('T41', '-', 'c'), ('T42', 'prepared', 'c'), ('T43', 'food', 'c'), ('T44', '.', 'c')]\n",
      "\n",
      "[('T0', 'It', 'c'), ('T1', 'may', 'c'), ('T2', 'in', 'c'), ('T3', 'many', 'c'), ('T4', 'cases', 'c'), ('T5', ',', 'c'), ('T6', 'not', 'c'), ('T7', 'be', 'c'), ('T8', 'true', 'c'), ('T9', 'but', 'c'), ('T10', 'we', 'c'), ('T11', 'can', 'c'), ('T12', 'suspect', 'c'), ('T13', 'that', 'c'), ('T14', 'most', 'c'), ('T15', 'of', 'c'), ('T16', 'them', 'c'), ('T17', 'wanted', 'c'), ('T18', 'to', 'c'), ('T19', 'become', 'c'), ('T20', 'a', 'c'), ('T21', 'celebrity', 'c'), ('T22', 'and', 'c'), ('T23', 'they', 'c'), ('T24', 'had', 'c'), ('T25', 'to', 'c'), ('T26', 'know', 'c'), ('T27', 'there', 'c'), ('T28', 'is', 'c'), ('T29', 'no', 'c'), ('T30', 'private', 'c'), ('T31', 'life', 'c'), ('T32', 'unseparately', 'c'), ('T33', 'contected', 'i'), ('T34', 'to', 'c'), ('T35', 'it', 'c'), ('T36', '.', 'c')]\n",
      "\n",
      "[('T0', 'Another', 'c'), ('T1', 'succesfull', 'i'), ('T2', 'carreer', 'i'), ('T3', 'such', 'c'), ('T4', 'as', 'c'), ('T5', ',', 'c'), ('T6', 'film', 'c'), ('T7', 'stars', 'c'), ('T8', 'must', 'c'), ('T9', 'also', 'c'), ('T10', 'be', 'c'), ('T11', 'balanced', 'c'), ('T12', '.', 'c')]\n",
      "\n",
      "[('T0', 'You', 'c'), ('T1', 'must', 'c'), ('T2', 'control', 'c'), ('T3', 'your', 'c'), ('T4', 'behaviour', 'c'), ('T5', 'and', 'c'), ('T6', 'try', 'c'), ('T7', 'not', 'c'), ('T8', 'implove', 'c'), ('T9', 'so', 'c'), ('T10', 'much', 'c'), ('T11', 'yourself', 'c'), ('T12', '.', 'c')]\n",
      "\n",
      "[('T0', 'Well', 'c'), ('T1', ',', 'c'), ('T2', 'you', 'c'), ('T3', 'can', 'c'), ('T4', 'look', 'c'), ('T5', 'at', 'c'), ('T6', 'your', 'c'), ('T7', 'house', 'c'), ('T8', '...', 'c')]\n",
      "\n",
      "[('T0', 'But', 'c'), ('T1', 'not', 'c'), ('T2', 'in', 'c'), ('T3', 'the', 'i'), ('T4', 'strange', 'c'), ('T5', 'way', 'c'), ('T6', 'you', 'c'), ('T7', 'can', 'c'), ('T8', 'imagine', 'c'), ('T9', '...', 'c')]\n",
      "\n",
      "[('T0', '!', 'c')]\n",
      "\n",
      "[('T0', 'I', 'c'), ('T1', 'felt', 'c'), ('T2', 'how', 'c'), ('T3', 'luck', 'i'), ('T4', 'I', 'c'), ('T5', 'was', 'c'), ('T6', '.', 'c')]\n",
      "\n",
      "[('T0', 'I', 'c'), ('T1', 'have', 'c'), ('T2', 'asked', 'i'), ('T3', 'to', 'c'), ('T4', 'write', 'c'), ('T5', 'a', 'c'), ('T6', 'composition', 'c'), ('T7', 'regarding', 'c'), ('T8', 'the', 'c'), ('T9', 'questionit', 'i'), ('T10', 'is', 'c'), ('T11', 'believed', 'c'), ('T12', 'that', 'c'), ('T13', 'shopping', 'c'), ('T14', 'is', 'c'), ('T15', 'not', 'c'), ('T16', 'always', 'c'), ('T17', 'enjoyable', 'c'), ('T18', '.', 'c')]\n",
      "\n",
      "[('T0', 'Please', 'c'), ('T1', ',', 'c'), ('T2', 'if', 'c'), ('T3', 'you', 'c'), ('T4', 'would', 'c'), ('T5', 'be', 'c'), ('T6', 'so', 'c'), ('T7', 'kind', 'c'), ('T8', 'and', 'c'), ('T9', 'mind', 'c'), ('T10', 'correct', 'c'), ('T11', 'this', 'c'), ('T12', 'mistake', 'c'), ('T13', '.', 'c'), ('T14', 'I', 'c'), ('T15', 'would', 'c'), ('T16', 'be', 'c'), ('T17', 'gratefull', 'i'), ('T18', 'if', 'c'), ('T19', 'you', 'c'), ('T20', 'could', 'c'), ('T21', 'correct', 'c'), ('T22', 'too', 'c'), ('T23', 'that', 'c'), ('T24', 'show', 'c'), ('T25', 'start', 'i'), ('T26', 'at', 'c'), ('T27', '20.15', 'c'), ('T28', 'not', 'c'), ('T29', 'on', 'c'), ('T30', '19.30', 'c'), ('T31', '.', 'c')]\n",
      "\n",
      "[('T0', 'Other', 'i'), ('T1', 'thing', 'c'), ('T2', 'was', 'c'), ('T3', 'that', 'c'), ('T4', 'there', 'c'), ('T5', 'was', 'i'), ('T6', 'no', 'c'), ('T7', 'discounts', 'c'), ('T8', 'available', 'c'), ('T9', ',', 'c'), ('T10', 'and', 'c'), ('T11', 'that', 'c'), ('T12', 'the', 'c'), ('T13', 'restaurant', 'c'), ('T14', 'which', 'c'), ('T15', 'I', 'c'), ('T16', 'wanted', 'c'), ('T17', 'to', 'c'), ('T18', 'visit', 'c'), ('T19', 'after', 'c'), ('T20', 'the', 'c'), ('T21', 'show', 'c'), ('T22', 'was', 'c'), ('T23', 'closed', 'c'), ('T24', 'because', 'c'), ('T25', 'of', 'c'), ('T26', 'the', 'c'), ('T27', 'main', 'c'), ('T28', 'cooker', 'i'), ('T29', 'sikness', 'i'), ('T30', '.', 'c')]\n",
      "\n",
      "[('T0', 'We', 'c'), ('T1', 'put', 'i'), ('T2', 'off', 'c'), ('T3', 'the', 'c'), ('T4', 'lights', 'c'), ('T5', 'and', 'c'), ('T6', 'then', 'c'), ('T7', 'the', 'c'), ('T8', 'door', 'c'), ('T9', 'opened', 'c'), ('T10', 'and', 'c'), ('T11', 'I', 'c'), ('T12', 'just', 'c'), ('T13', 'heard', 'c'), ('T14', '\"', 'c'), ('T15', 'What', 'c'), ('T16', 'the', 'c'), ('T17', 'hell', 'c'), ('T18', 'is', 'c'), ('T19', 'going', 'c'), ('T20', 'on', 'c'), ('T21', '?', 'c'), ('T22', '!', 'c'), ('T23', '!', 'c'), ('T24', '\"', 'c')]\n",
      "\n",
      "[('T0', 'CONCERNING', 'c'), ('T1', 'THE', 'c'), ('T2', 'ACCOMMODATION', 'c'), ('T3', '-', 'c'), ('T4', 'PLEASE', 'c'), ('T5', 'BE', 'c'), ('T6', 'INFORMED', 'c'), ('T7', 'THAT', 'c'), ('T8', 'I', 'c'), ('T9', 'WOULD', 'c'), ('T10', 'PREFER', 'c'), ('T11', 'THE', 'c'), ('T12', 'LOG', 'c'), ('T13', 'CABIN', 'c'), ('T14', 'AS', 'c'), ('T15', 'IN', 'c'), ('T16', 'THE', 'c'), ('T17', 'MEANTIME', 'c'), ('T18', 'I', 'c'), ('T19', 'SHOULD', 'c'), ('T20', 'WORK', 'c'), ('T21', 'ON', 'c'), ('T22', 'MY', 'c'), ('T23', 'LAPTOP', 'c'), ('T24', ',', 'c'), ('T25', 'PREPARING', 'c'), ('T26', 'SOME', 'c'), ('T27', 'FINANCIAL', 'c'), ('T28', 'REPORTS', 'c'), ('T29', 'SO', 'c'), ('T30', 'THE', 'c'), ('T31', 'ELECTRICITY', 'c'), ('T32', 'WILL', 'c'), ('T33', 'BE', 'c'), ('T34', 'NEEDED', 'c'), ('T35', '.', 'c'), ('T36', 'I', 'c'), ('T37', 'THINK', 'c'), ('T38', 'THAT', 'c'), ('T39', 'THE', 'c'), ('T40', 'LOG', 'c'), ('T41', 'CABIN', 'c'), ('T42', 'WILL', 'c'), ('T43', 'BE', 'c'), ('T44', 'MORE', 'c'), ('T45', 'COMFORTABLE', 'c'), ('T46', 'AT', 'i'), ('T47', 'ALL', 'c'), ('T48', '.', 'c'), ('T49', 'REGARDING', 'c'), ('T50', 'TWO', 'c'), ('T51', 'ACTIVITIES', 'c'), ('T52', '-', 'c'), ('T53', 'I', 'c'), ('T54', 'HAVE', 'c'), ('T55', 'CHOOSEN', 'i'), ('T56', 'TENNIS', 'c'), ('T57', 'AND', 'c'), ('T58', 'PHOTOGRAPHY', 'c'), ('T59', '.', 'c')]\n",
      "\n",
      "[('T0', 'I', 'c'), ('T1', 'think', 'c'), ('T2', 'they', 'c'), ('T3', 'are', 'c'), ('T4', 'so', 'c'), ('T5', 'hospitality', 'i'), ('T6', 'and', 'c'), ('T7', 'warm', 'c'), ('T8', 'with', 'c'), ('T9', 'people', 'c'), ('T10', 'around', 'c'), ('T11', 'them', 'c'), ('T12', '.', 'c'), ('T13', 'You', 'c'), ('T14', 'know', 'c'), ('T15', '!', 'c'), ('T16', 'because', 'c'), ('T17', 'of', 'c'), ('T18', 'these', 'i'), ('T19', 'are', 'c'), ('T20', 'change', 'i'), ('T21', 'all', 'i'), ('T22', 'my', 'i'), ('T23', 'opinions', 'i'), ('T24', 'about', 'i'), ('T25', 'self', 'i'), ('T26', '-', 'i'), ('T27', 'important', 'i'), ('T28', 'behavior', 'i'), ('T29', 'of', 'i'), ('T30', 'superstar', 'i'), ('T31', '.', 'c')]\n",
      "\n",
      "[('T0', 'The', 'i'), ('T1', 'most', 'c'), ('T2', 'appreciated', 'c'), ('T3', 'was', 'c'), ('T4', 'they', 'i'), ('T5', \"'ve\", 'i'), ('T6', 'given', 'i'), ('T7', 'me', 'c'), ('T8', 'their', 'c'), ('T9', 'latate', 'i'), ('T10', 'CD', 'c'), ('T11', 'with', 'c'), ('T12', 'signature', 'i'), ('T13', '.', 'c'), ('T14', 'Marvellous', 'c'), ('T15', 'thing', 'c'), ('T16', '!', 'c'), ('T17', 'and', 'i'), ('T18', 'of', 'c'), ('T19', 'couse', 'i'), ('T20', 'as', 'c'), ('T21', 'you', 'c'), ('T22', 'have', 'c'), ('T23', 'seen', 'c'), ('T24', 'it', 'c'), ('T25', 'I', 'c'), ('T26', \"'ve\", 'c'), ('T27', 'given', 'c'), ('T28', 'one', 'c'), ('T29', 'to', 'c'), ('T30', 'you', 'c'), ('T31', '.', 'c')]\n",
      "\n",
      "[('T0', 'When', 'c'), ('T1', 'I', 'c'), ('T2', 'was', 'c'), ('T3', 'a', 'c'), ('T4', 'child', 'c'), ('T5', 'I', 'c'), ('T6', 'had', 'c'), ('T7', 'different', 'c'), ('T8', 'entertainments', 'i'), ('T9', 'that', 'i'), ('T10', 'the', 'c'), ('T11', 'actually', 'i'), ('T12', 'children', 'c'), ('T13', 'had', 'i'), ('T14', \"n't\", 'i'), ('T15', '.', 'c')]\n",
      "\n",
      "[('T0', 'Also', 'c'), ('T1', ',', 'c'), ('T2', 'I', 'c'), ('T3', 'felt', 'c'), ('T4', 'wonderful', 'c'), ('T5', 'when', 'c'), ('T6', 'I', 'c'), ('T7', 'saw', 'c'), ('T8', ',', 'c'), ('T9', 'that', 'c'), ('T10', 'the', 'c'), ('T11', 'concert', 'c'), ('T12', 'had', 'i'), ('T13', 'succesed', 'i'), ('T14', 'and', 'c'), ('T15', 'every', 'c'), ('T16', 'day', 'c'), ('T17', 'was', 'i'), ('T18', 'croweded', 'i'), ('T19', '.', 'c')]\n",
      "\n",
      "[('T0', 'The', 'c'), ('T1', 'show', 'c'), ('T2', 'is', 'c'), ('T3', 'on', 'c'), ('T4', 'Tuesday', 'c'), ('T5', 'the', 'c'), ('T6', '14th', 'c'), ('T7', 'between', 'c'), ('T8', '10.00-', 'i'), ('T9', '13.00', 'i'), ('T10', '.', 'c')]\n",
      "\n",
      "[('T0', 'And', 'c'), ('T1', 'all', 'c'), ('T2', 'of', 'c'), ('T3', 'it', 'c'), ('T4', 'is', 'c'), ('T5', 'free', 'c'), ('T6', ',', 'c')]\n",
      "\n",
      "[('T0', 'We', 'c'), ('T1', 'are', 'c'), ('T2', 'sentimental', 'c'), ('T3', 'and', 'c'), ('T4', 'we', 'c'), ('T5', 'have', 'c'), ('T6', 'memory', 'i'), ('T7', 'from', 'c'), ('T8', 'the', 'c'), ('T9', 'past', 'c'), ('T10', '.', 'c'), ('T11', 'I', 'c'), ('T12', 'think', 'c'), ('T13', 'it', 'c'), ('T14', \"'s\", 'c'), ('T15', 'a', 'c'), ('T16', 'part', 'c'), ('T17', 'of', 'c'), ('T18', 'our', 'c'), ('T19', 'life.lf', 'i')]\n",
      "\n",
      "[('T0', 'Eventhrough', 'i'), ('T1', 'we', 'c'), ('T2', 'could', 'c'), ('T3', 'spend', 'c'), ('T4', 'our', 'c'), ('T5', 'holiday', 'c'), ('T6', 'there', 'c'), ('T7', 'but', 'c'), ('T8', 'with', 'c'), ('T9', 'my', 'c'), ('T10', 'mum', 'c'), ('T11', 'and', 'c'), ('T12', 'I', 'c'), ('T13', 'just', 'c'), ('T14', 'found', 'c'), ('T15', 'that', 'c'), ('T16', 'it', 'c'), ('T17', 'was', 'c'), ('T18', 'fantasy', 'c'), ('T19', 'time', 'c'), ('T20', 'which', 'c'), ('T21', 'I', 'c'), ('T22', 'had', 'c'), ('T23', 'never', 'c'), ('T24', 'though', 'i'), ('T25', 'about', 'c'), ('T26', 'it', 'c'), ('T27', '.', 'c')]\n",
      "\n",
      "[('T0', 'In', 'c'), ('T1', 'addition', 'c'), ('T2', ',', 'c'), ('T3', 'I', 'c'), ('T4', 'think', 'c'), ('T5', 'you', 'c'), ('T6', 'have', 'i'), ('T7', 'to', 'i'), ('T8', 'take', 'i'), ('T9', 'also', 'i'), ('T10', 'plans', 'c'), ('T11', 'from', 'c'), ('T12', 'the', 'c'), ('T13', 'school', 'c'), ('T14', \"'s\", 'c'), ('T15', 'Nursery', 'i'), ('T16', 'as', 'c'), ('T17', 'there', 'c'), ('T18', 'are', 'c'), ('T19', 'a', 'c'), ('T20', 'lot', 'c'), ('T21', 'of', 'c'), ('T22', 'facilities', 'c'), ('T23', 'for', 'c'), ('T24', 'the', 'c'), ('T25', 'children', 'c'), ('T26', ',', 'c'), ('T27', 'and', 'c'), ('T28', 'the', 'c'), ('T29', 'parents', 'c'), ('T30', 'can', 'c'), ('T31', 'recognise', 'i'), ('T32', 'that', 'c'), ('T33', 'the', 'c'), ('T34', 'teachers', 'c'), ('T35', 'are', 'c'), ('T36', 'very', 'c'), ('T37', 'good', 'c'), ('T38', '.', 'c')]\n",
      "\n",
      "[('T0', 'In', 'c'), ('T1', 'the', 'i'), ('T2', 'other', 'i'), ('T3', 'word', 'i'), ('T4', ',', 'c'), ('T5', 'I', 'c'), ('T6', 'can', 'c'), ('T7', 'also', 'c'), ('T8', 'receive', 'c'), ('T9', 'their', 'c'), ('T10', 'reply', 'c'), ('T11', 'soon', 'i'), ('T12', '.', 'c')]\n",
      "\n",
      "[('T0', 'They', 'c'), ('T1', 'can', 'c'), ('T2', 'wear', 'c'), ('T3', 'different', 'c'), ('T4', 'kinds', 'c'), ('T5', 'of', 'c'), ('T6', 'clothing', 'c'), ('T7', ',', 'c'), ('T8', 'like', 'c'), ('T9', 'they', 'c'), ('T10', 'design', 'c'), ('T11', 'for', 'c'), ('T12', 'themselves', 'c'), ('T13', 'or', 'c'), ('T14', 'those', 'c'), ('T15', 'like', 'c'), ('T16', 'history', 'c'), ('T17', 'can', 'c'), ('T18', 'wear', 'c'), ('T19', 'those', 'c'), ('T20', 'of', 'c'), ('T21', 'victorian', 'i'), ('T22', 'time', 'i'), ('T23', 'and', 'c'), ('T24', 'in', 'c'), ('T25', 'different', 'c'), ('T26', 'countries', 'c'), ('T27', '.', 'c')]\n",
      "\n",
      "[('T0', 'As', 'c'), ('T1', 'different', 'c'), ('T2', 'kinds', 'c'), ('T3', 'of', 'c'), ('T4', 'farbics', 'i'), ('T5', 'will', 'c'), ('T6', 'be', 'c'), ('T7', 'invented', 'c'), ('T8', '&', 'i'), ('T9', 'the', 'c'), ('T10', 'clothings', 'c'), ('T11', 'will', 'c'), ('T12', 'never', 'c'), ('T13', 'be', 'c'), ('T14', 'just', 'c'), ('T15', 'cotton', 'c'), ('T16', 'as', 'c'), ('T17', 'always', 'c'), ('T18', 'been', 'c'), ('T19', 'seen', 'c'), ('T20', '.', 'c')]\n",
      "\n",
      "[('T0', 'On', 'c'), ('T1', 'the', 'c'), ('T2', 'one', 'c'), ('T3', 'hand', 'c'), ('T4', 'Internet', 'i'), ('T5', 'is', 'c'), ('T6', 'often', 'c'), ('T7', 'used', 'c'), ('T8', 'for', 'c'), ('T9', 'entertaiment', 'i'), ('T10', ',', 'c'), ('T11', 'but', 'c'), ('T12', 'on', 'c'), ('T13', 'the', 'c'), ('T14', 'other', 'c'), ('T15', 'hand', 'c'), ('T16', 'it', 'c'), ('T17', \"'s\", 'c'), ('T18', 'also', 'c'), ('T19', 'used', 'c'), ('T20', 'in', 'c'), ('T21', 'different', 'c'), ('T22', 'business', 'c'), ('T23', 'and', 'c'), ('T24', 'education', 'c'), ('T25', 'process', 'i'), ('T26', '!', 'c')]\n",
      "\n",
      "[('T0', 'Unconected', 'i'), ('T1', 'part', 'c'), ('T2', 'of', 'c'), ('T3', 'every', 'i'), ('T4', 'people', 'c'), ('T5', 'lifes', 'i'), ('T6', 'is', 'c'), ('T7', 'shopping', 'c'), ('T8', '.', 'c')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in final_unmatched_multiged_dev:\n",
    "    print(sentence)\n",
    "    print()\n",
    "\n",
    "#4 cases do not match because the sentence is a single token !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "4b6720d5-7503-4dd2-a1ea-5923e02bc9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found both in sentence 17187:\n",
      "['doc283.xml', 'T0', 'Firs', 'i', 'First', 'SX']\n",
      "['doc283.xml', 'T1', 'of', 'c', '', '']\n",
      "['doc283.xml', 'T2', 'all', 'c', '', '']\n",
      "['doc283.xml', 'T3', 'Internet', 'c', '', '']\n",
      "['doc283.xml', 'T4', 'gives', 'c', '', '']\n",
      "['doc283.xml', 'T5', 'you', 'c', '', '']\n",
      "['doc283.xml', 'T6', 'an', 'i', '', 'UD']\n",
      "['doc283.xml', 'T7', 'access', 'c', '', '']\n",
      "['doc283.xml', 'T8', 'to', 'c', '', '']\n",
      "['doc283.xml', 'T9', 'a', 'c', '', '']\n",
      "['doc283.xml', 'T10', 'great', 'c', '', '']\n",
      "['doc283.xml', 'T11', 'number', 'c', '', '']\n",
      "['doc283.xml', 'T12', 'of', 'c', '', '']\n",
      "['doc283.xml', 'T13', 'different', 'c', '', '']\n",
      "['doc283.xml', 'T14', 'information', 'i', '', 'RN']\n",
      "['doc283.xml', 'T15', '.', 'c', '', '']\n",
      "['doc283.xml', 'T0', 'On', 'c', '', '']\n",
      "['doc283.xml', 'T1', 'the', 'c', '', '']\n",
      "['doc283.xml', 'T2', 'one', 'c', '', '']\n",
      "['doc283.xml', 'T3', 'hand', 'c', '', '']\n",
      "['doc283.xml', 'T4', 'Internet', 'c', '', '']\n",
      "['doc283.xml', 'T5', 'is', 'c', '', '']\n",
      "['doc283.xml', 'T6', 'often', 'c', '', '']\n",
      "['doc283.xml', 'T7', 'used', 'c', '', '']\n",
      "['doc283.xml', 'T8', 'for', 'c', '', '']\n",
      "['doc283.xml', 'T9', 'entertaiment', 'i', 'entertainment', 'S']\n",
      "['doc283.xml', 'T10', ',', 'c', '', '']\n",
      "['doc283.xml', 'T11', 'but', 'c', '', '']\n",
      "['doc283.xml', 'T12', 'on', 'c', '', '']\n",
      "['doc283.xml', 'T13', 'the', 'c', '', '']\n",
      "['doc283.xml', 'T14', 'other', 'c', '', '']\n",
      "['doc283.xml', 'T15', 'hand', 'c', '', '']\n",
      "['doc283.xml', 'T16', 'it', 'c', '', '']\n",
      "['doc283.xml', 'T17', \"'s\", 'c', '', '']\n",
      "['doc283.xml', 'T18', 'also', 'c', '', '']\n",
      "['doc283.xml', 'T19', 'used', 'c', '', '']\n",
      "['doc283.xml', 'T20', 'in', 'c', '', '']\n",
      "['doc283.xml', 'T21', 'different', 'c', '', '']\n",
      "['doc283.xml', 'T22', 'business', 'c', '', '']\n",
      "['doc283.xml', 'T23', 'and', 'c', '', '']\n",
      "['doc283.xml', 'T24', 'education', 'c', '', '']\n",
      "['doc283.xml', 'T25', '!', 'i', '.', 'RP']\n",
      "\n",
      "Found both in sentence 17188:\n",
      "['doc283.xml', 'T0', 'On', 'c', '', '']\n",
      "['doc283.xml', 'T1', 'the', 'c', '', '']\n",
      "['doc283.xml', 'T2', 'one', 'c', '', '']\n",
      "['doc283.xml', 'T3', 'hand', 'c', '', '']\n",
      "['doc283.xml', 'T4', 'Internet', 'c', '', '']\n",
      "['doc283.xml', 'T5', 'is', 'c', '', '']\n",
      "['doc283.xml', 'T6', 'often', 'c', '', '']\n",
      "['doc283.xml', 'T7', 'used', 'c', '', '']\n",
      "['doc283.xml', 'T8', 'for', 'c', '', '']\n",
      "['doc283.xml', 'T9', 'entertaiment', 'i', 'entertainment', 'S']\n",
      "['doc283.xml', 'T10', ',', 'c', '', '']\n",
      "['doc283.xml', 'T11', 'but', 'c', '', '']\n",
      "['doc283.xml', 'T12', 'on', 'c', '', '']\n",
      "['doc283.xml', 'T13', 'the', 'c', '', '']\n",
      "['doc283.xml', 'T14', 'other', 'c', '', '']\n",
      "['doc283.xml', 'T15', 'hand', 'c', '', '']\n",
      "['doc283.xml', 'T16', 'it', 'c', '', '']\n",
      "['doc283.xml', 'T17', \"'s\", 'c', '', '']\n",
      "['doc283.xml', 'T18', 'also', 'c', '', '']\n",
      "['doc283.xml', 'T19', 'used', 'c', '', '']\n",
      "['doc283.xml', 'T20', 'in', 'c', '', '']\n",
      "['doc283.xml', 'T21', 'different', 'c', '', '']\n",
      "['doc283.xml', 'T22', 'business', 'c', '', '']\n",
      "['doc283.xml', 'T23', 'and', 'c', '', '']\n",
      "['doc283.xml', 'T24', 'education', 'c', '', '']\n",
      "['doc283.xml', 'T25', '!', 'i', '.', 'RP']\n",
      "\n",
      "Found both in sentence 17189:\n",
      "['doc283.xml', 'T0', 'On', 'c', '', '']\n",
      "['doc283.xml', 'T1', 'the', 'c', '', '']\n",
      "['doc283.xml', 'T2', 'one', 'c', '', '']\n",
      "['doc283.xml', 'T3', 'hand', 'c', '', '']\n",
      "['doc283.xml', 'T4', 'Internet', 'c', '', '']\n",
      "['doc283.xml', 'T5', 'is', 'c', '', '']\n",
      "['doc283.xml', 'T6', 'often', 'c', '', '']\n",
      "['doc283.xml', 'T7', 'used', 'c', '', '']\n",
      "['doc283.xml', 'T8', 'for', 'c', '', '']\n",
      "['doc283.xml', 'T9', 'entertaiment', 'i', 'entertainment', 'S']\n",
      "['doc283.xml', 'T10', ',', 'c', '', '']\n",
      "['doc283.xml', 'T11', 'but', 'c', '', '']\n",
      "['doc283.xml', 'T12', 'on', 'c', '', '']\n",
      "['doc283.xml', 'T13', 'the', 'c', '', '']\n",
      "['doc283.xml', 'T14', 'other', 'c', '', '']\n",
      "['doc283.xml', 'T15', 'hand', 'c', '', '']\n",
      "['doc283.xml', 'T16', 'it', 'c', '', '']\n",
      "['doc283.xml', 'T17', \"'s\", 'c', '', '']\n",
      "['doc283.xml', 'T18', 'also', 'c', '', '']\n",
      "['doc283.xml', 'T19', 'used', 'c', '', '']\n",
      "['doc283.xml', 'T20', 'in', 'c', '', '']\n",
      "['doc283.xml', 'T21', 'different', 'c', '', '']\n",
      "['doc283.xml', 'T22', 'business', 'c', '', '']\n",
      "['doc283.xml', 'T23', 'and', 'c', '', '']\n",
      "['doc283.xml', 'T24', 'education', 'c', '', '']\n",
      "['doc283.xml', 'T25', '!', 'i', '.', 'RP']\n",
      "['doc283.xml', 'T0', 'With', 'c', '', '']\n",
      "['doc283.xml', 'T1', 'the', 'c', '', '']\n",
      "['doc283.xml', 'T2', 'help', 'c', '', '']\n",
      "['doc283.xml', 'T3', 'of', 'c', '', '']\n",
      "['doc283.xml', 'T4', 'Internet', 'i', 'internet', 'RP']\n",
      "['doc283.xml', 'T5', 'technologies', 'c', '', '']\n",
      "['doc283.xml', 'T6', 'I', 'c', '', '']\n",
      "['doc283.xml', 'T7', 'communicate', 'c', '', '']\n",
      "['doc283.xml', 'T8', 'with', 'c', '', '']\n",
      "['doc283.xml', 'T9', 'other', 'c', '', '']\n",
      "['doc283.xml', 'T10', 'people', 'c', '', '']\n",
      "['doc283.xml', 'T11', 'from', 'c', '', '']\n",
      "['doc283.xml', 'T12', 'all', 'c', '', '']\n",
      "['doc283.xml', 'T13', 'over', 'c', '', '']\n",
      "['doc283.xml', 'T14', 'the', 'c', '', '']\n",
      "['doc283.xml', 'T15', 'world', 'c', '', '']\n",
      "['doc283.xml', 'T16', 'it', 'c', '', '']\n",
      "['doc283.xml', 'T17', 'gives', 'c', '', '']\n",
      "['doc283.xml', 'T18', 'me', 'c', '', '']\n",
      "['doc283.xml', 'T19', 'chance', 'c', '', '']\n",
      "['doc283.xml', 'T20', 'to', 'c', '', '']\n",
      "['doc283.xml', 'T21', 'improve', 'c', '', '']\n",
      "['doc283.xml', 'T22', 'my', 'c', '', '']\n",
      "['doc283.xml', 'T23', 'English', 'c', '', '']\n",
      "['doc283.xml', 'T24', '.', 'c', '', '']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_tokens = {'business','hand','On'}\n",
    "\n",
    "for idx, sentence in enumerate(new_unmatched_sentences_fce):\n",
    "    tokens_in_sentence = {row[2] for row in sentence}\n",
    "    if target_tokens.issubset(tokens_in_sentence):\n",
    "        print(f\"Found both in sentence {idx}:\")\n",
    "        for r in sentence:\n",
    "            print(r)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3d08f551-fbd2-4bf9-9c9a-1addc0942804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = ['filename', 'token', 'correction', 'error_type', 'gold_label']\n",
    "\n",
    "# with open('processed_fce.tsv', 'w', newline='', encoding='utf-8') as f_out:\n",
    "#     writer = csv.writer(f_out, delimiter='\\t')\n",
    "#     writer.writerow(headers)\n",
    "\n",
    "#     for sentence in dev_matches_spacy_idk:\n",
    "#         for row in sentence:\n",
    "#             row_without_id = [row[0], row[2], row[4], row[5], row[6]]\n",
    "#             writer.writerow(row_without_id)\n",
    "#         writer.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a014671-69b1-4670-a5cd-fc1fe23b9c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
